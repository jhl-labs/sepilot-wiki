---
title: Qwen 3.5
author: SEPilot AI
status: published
tags: [Qwen, LLM, 멀티모달, MoE, 벤치마크]
redirect_from:
  - qwen3-5
order: 5
related_docs: ["gemini-3-1.md", "glm-5.md"]
---

## 1. 개요
**Qwen 3.5**는 **Alibaba**에서 발표한 최신 대규모 언어 모델(LLM)입니다. **Gated DeltaNet + Mixture‑of‑Experts(MoE)** 아키텍처를 채택하여, 전체 397B 파라미터 중 17B만 활성화하는 방식으로 높은 성능과 효율성을 동시에 달성합니다.

- **주요 목표** – 텍스트·이미지·비디오를 하나의 모델로 처리하면서, 코딩 에이전트·검색 에이전트 등 도구 활용 능력까지 갖춘 범용 AI 모델.
- **주요 적용 분야** – 챗봇, 코딩 에이전트, 문서·이미지 분석, 다국어 번역, 의료 영상 분석 등.

### 모델 사양
| 항목 | 내용 |
|------|------|
| **전체 파라미터** | 397B (3,970억) |
| **활성 파라미터** | 17B (A17B) |
| **아키텍처** | Gated DeltaNet + MoE (512 experts, 10 routed + 1 shared) |
| **컨텍스트 길이** | 기본 262,144 토큰, 최대 1,010,000 토큰까지 확장 |
| **지원 언어** | 201개 언어 및 방언 |

> **쉽게 말해**: MoE(Mixture‑of‑Experts)는 전문가 여러 명 중 필요한 전문가만 골라 쓰는 방식입니다. 512명의 전문가 중 매번 10명만 활성화하기 때문에, 거대한 모델이지만 실제 연산량은 17B 모델 수준으로 유지됩니다.

---

## 2. 모델 아키텍처
1. **Gated DeltaNet** – 기존 Transformer의 attention 메커니즘을 개선한 구조로, 긴 문맥에서도 메모리 효율이 좋습니다.
2. **Mixture‑of‑Experts (MoE)** – 512개의 전문가(expert) 네트워크 중 10개를 라우팅하고, 1개의 공유 전문가를 항상 활성화합니다. 이 덕분에 전체 397B 파라미터의 지식을 활용하면서도 실제 연산은 17B 수준으로 유지됩니다.
3. **멀티모달 입력 처리** – 텍스트·이미지·비디오를 동일한 토큰 공간으로 변환하여 하나의 모델에서 처리합니다.
4. **초장문 컨텍스트** – 기본 262K 토큰, 최대 약 100만 토큰까지 처리 가능하여 대규모 코드베이스나 긴 문서 분석에 유리합니다.

---

## 3. 학습 데이터 및 방법
| 구분 | 내용 |
|------|------|
| **사전학습** | 다국어 텍스트, 이미지-텍스트 쌍, 코드 데이터로 멀티모달 사전학습 |
| **후처리** | RLHF(인간 피드백 기반 강화학습)를 통한 미세조정 |
| **지원 언어** | 201개 언어 및 방언 (다국어 벤치마크에서 최상위권 성능) |
| **효율성 최적화** | MoE 라우팅, Mixed‑Precision(BF16) |

---

## 4. 주요 기능 및 특징
| 기능 | 설명 |
|------|------|
| **자연어 이해·생성** | MMLU‑Pro 87.8%, SuperGPQA 70.4% 등 지식 벤치마크에서 GPT‑5.2에 근접하는 성능 |
| **코딩 에이전트** | SWE‑bench Verified 76.4%, LiveCodeBench v6 83.6% 등 실제 코드 수정·생성 능력 검증 |
| **멀티모달 처리** | 이미지·비디오 이해, 문서 OCR, 공간 인식 등 다양한 비전 태스크 지원 |
| **도구·에이전트 활용** | BFCL‑V4 72.9%, MCP‑Mark 46.1% 등 도구 호출 및 에이전트 작업에서 강점 |
| **초장문 처리** | 최대 100만 토큰 컨텍스트로 대규모 코드베이스·문서 분석 가능 |
| **다국어 지원** | 201개 언어 지원, MMMLU 88.5%, NOVA‑63 59.1%로 다국어 벤치마크 최상위권 |

---

## 5. 벤치마크 성능

> **출처** – [Hugging Face Model Card](https://huggingface.co/Qwen/Qwen3.5-397B-A17B). 비교 모델: GPT‑5.2, Claude 4.5 Opus, Gemini‑3 Pro, Qwen3‑Max‑Thinking, K2.5‑1T‑A32B.

### 5‑1. 언어 벤치마크

#### 지식 (Knowledge)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| MMLU‑Pro | **87.8** | 87.4 | 89.5 | 89.8 |
| MMLU‑Redux | **94.9** | 95.0 | 95.6 | 95.9 |
| SuperGPQA | **70.4** | 67.9 | 70.6 | 74.0 |
| C‑Eval | **93.0** | 90.5 | 92.2 | 93.4 |

#### 지시 수행 (Instruction Following)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| IFEval | **92.6** | 94.8 | 90.9 | 93.5 |
| IFBench | **76.5** | 75.4 | 58.0 | 70.4 |
| MultiChallenge | **67.6** | 57.9 | 54.2 | 64.2 |

#### STEM (과학·기술·공학·수학)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| GPQA | **88.4** | 92.4 | 87.0 | 91.9 |
| HLE | **28.7** | 35.5 | 30.8 | 37.5 |
| HLE‑Verified | **37.6** | 43.3 | 38.8 | 48.0 |

#### 추론 (Reasoning)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| LiveCodeBench v6 | **83.6** | 87.7 | 84.8 | 90.7 |
| HMMT Feb 25 | **94.8** | 99.4 | 92.9 | 97.3 |
| HMMT Nov 25 | **92.7** | 100 | 93.3 | 93.3 |
| IMOAnswerBench | **80.9** | 86.3 | 84.0 | 83.3 |
| AIME26 | **91.3** | 96.7 | 93.3 | 90.6 |

#### 긴 문맥 (Long Context)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| AA‑LCR | **68.7** | 72.7 | 74.0 | 70.7 |
| LongBench v2 | **63.2** | 54.5 | 64.4 | 68.2 |

#### 일반 에이전트 (General Agent)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| BFCL‑V4 | **72.9** | 63.1 | 77.5 | 72.5 |
| TAU2‑Bench | **86.7** | 87.1 | 91.6 | 85.4 |
| VITA‑Bench | **49.7** | 38.2 | 56.3 | 51.6 |
| DeepPlanning | **34.3** | 44.6 | 33.9 | 23.3 |
| Tool Decathlon | **38.3** | 43.8 | 43.5 | 36.4 |
| MCP‑Mark | **46.1** | 57.5 | 42.3 | 53.9 |

#### 검색 에이전트 (Search Agent)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| HLE w/ tool | **48.3** | 45.5 | 43.4 | 45.8 |
| BrowseComp | **69.0** | 65.8 | 67.8 | 59.2 |
| BrowseComp‑zh | **70.3** | 76.1 | 62.4 | 66.8 |
| WideSearch | **74.0** | 76.8 | 76.4 | 68.0 |

#### 코딩 에이전트 (Coding Agent)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| SWE‑bench Verified | **76.4** | 80.0 | 80.9 | 76.2 |
| SWE‑bench Multilingual | **69.3** | 72.0 | 77.5 | 65.0 |
| SecCodeBench | **68.3** | 68.7 | 68.6 | 62.4 |
| Terminal Bench 2 | **52.5** | 54.0 | 59.3 | 54.2 |

#### 다국어 (Multilingualism)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| MMMLU | **88.5** | 89.5 | 90.1 | 90.6 |
| MMLU‑ProX | **84.7** | 83.7 | 85.7 | 87.7 |
| NOVA‑63 | **59.1** | 54.6 | 56.7 | 56.7 |
| INCLUDE | **85.6** | 87.5 | 86.2 | 90.5 |
| Global PIQA | **89.8** | 90.9 | 91.6 | 93.2 |
| PolyMATH | **73.3** | 62.5 | 79.0 | 81.6 |
| WMT24++ | **78.9** | 78.8 | 79.7 | 80.7 |
| MAXIFE | **88.2** | 88.4 | 79.2 | 87.5 |

### 5‑2. 비전‑언어 벤치마크

#### STEM 및 퍼즐
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| MMMU | **85.0** | 86.7 | 80.7 | 87.2 |
| MMMU‑Pro | **79.0** | 79.5 | 70.6 | 81.0 |
| MathVision | **88.6** | 83.0 | 74.3 | 86.6 |
| MathVista (mini) | **90.3** | 83.1 | 80.0 | 87.9 |
| We‑Math | **87.9** | 79.0 | 70.0 | 86.9 |
| DynaMath | **86.3** | 86.8 | 79.7 | 85.1 |
| ZEROBench | **12** | 9 | 3 | 10 |
| BabyVision | **52.3** | 34.4 | 14.2 | 49.7 |

#### 일반 시각 이해 (General VQA)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| RealWorldQA | **83.9** | 83.3 | 77.0 | 83.3 |
| MMStar | **83.8** | 77.1 | 73.2 | 83.1 |
| HallusionBench | **71.4** | 65.2 | 64.1 | 68.6 |
| MMBench EN | **93.7** | 88.2 | 89.2 | 93.7 |
| SimpleVQA | **67.1** | 55.8 | 65.7 | 73.2 |

#### 문서 이해·OCR
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| OmniDocBench1.5 | **90.8** | 85.7 | 87.7 | 88.5 |
| CharXiv (RQ) | **80.8** | 82.1 | 68.5 | 81.4 |
| MMLongBench‑Doc | **61.5** | — | 61.9 | 60.5 |
| CC‑OCR | **82.0** | 70.3 | 76.9 | 79.0 |
| AI2D TEST | **93.9** | 92.2 | 87.7 | 94.1 |
| OCRBench | **93.1** | 80.7 | 85.8 | 90.4 |

#### 공간 인식 (Spatial Intelligence)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| ERQA | **67.5** | 59.8 | 46.8 | 70.5 |
| CountBench | **97.2** | 91.9 | 90.6 | 97.3 |
| EmbSpatialBench | **84.5** | 81.3 | 75.7 | 61.2 |
| LingoQA | **81.6** | 68.8 | 78.8 | 72.8 |
| V* | **95.8** | 75.9 | 67.0 | 88.0 |

#### 비디오 이해
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| VideoMME (w/ sub) | **87.5** | 86.0 | 77.6 | 88.4 |
| VideoMME (w/o sub) | **83.7** | 85.8 | 81.4 | 87.7 |
| VideoMMMU | **84.7** | 85.9 | 84.4 | 87.6 |
| MLVU (M‑Avg) | **86.7** | 85.6 | 81.7 | 83.0 |
| MVBench | **77.6** | 78.1 | 67.2 | 74.1 |
| LVBench | **75.5** | 73.7 | 57.3 | 76.2 |

#### 비주얼 에이전트
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| ScreenSpot Pro | **65.6** | — | 45.7 | 72.7 |
| OSWorld‑Verified | **62.2** | 38.2 | 66.3 | — |
| AndroidWorld | **66.8** | — | — | — |

#### 의료 (Medical VQA)
| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |
|----------|----------|---------|-----------------|--------------|
| SLAKE | **79.9** | 76.9 | 76.4 | 81.3 |
| PMC‑VQA | **64.2** | 58.9 | 59.9 | 62.3 |
| MedXpertQA‑MM | **70.0** | 73.3 | 63.6 | 76.0 |

### 5‑3. 성능 요약
- **비전‑수학 분야 최강**: MathVision(88.6%), MathVista(90.3%), We‑Math(87.9%)에서 GPT‑5.2와 Gemini‑3 Pro를 앞섬.
- **문서·OCR 특화**: OmniDocBench(90.8%), OCRBench(93.1%), CC‑OCR(82.0%)에서 전 모델 대비 최고 성능.
- **공간 인식 우수**: V*(95.8%), CountBench(97.2%), EmbSpatialBench(84.5%)에서 압도적 차이.
- **다국어 강점**: NOVA‑63(59.1%), MAXIFE(88.2%)에서 전 모델 1위.
- **에이전트 능력**: IFBench(76.5%), MultiChallenge(67.6%)에서 지시 수행 능력이 돋보임.
- **추론·코딩은 GPT‑5.2에 비해 소폭 뒤처짐**: AIME26(91.3 vs 96.7), SWE‑bench Verified(76.4 vs 80.0).

---

## 6. 라이선스 및 데이터 사용권
| 항목 | 내용 | 비고 |
|------|------|------|
| **모델 코드·가중치** | Apache 2.0 | 상업적·비상업적 모두 사용 가능 |
| **텍스트 데이터** | CC‑BY 4.0, CC‑0, 자체 수집 | 상세 라이선스는 모델 카드 참고 |
| **코드 데이터** | MIT, Apache 2.0, GPL 등 | 개별 레포지터리 라이선스 확인 필요 |

---

## 7. 제한점 및 주의사항
- **추론 비용** – 397B 모델은 대규모 GPU 클러스터가 필요하므로, 개인 환경에서는 경량 파생 모델 사용을 권장합니다.
- **편향·안전성** – 대규모 웹 데이터 학습 특성상 성별·인종·문화 편향이 존재할 수 있습니다.
- **HLE 성능** – Humanity's Last Exam 벤치마크에서 28.7%로, GPT‑5.2(35.5%)·Gemini‑3 Pro(37.5%)에 비해 초고난이도 문제에서 약세를 보입니다.

---

## 8. 참고 자료
- **Hugging Face Model Card** – https://huggingface.co/Qwen/Qwen3.5-397B-A17B
- **Qwen 공식 블로그** – https://qwenlm.github.io/blog/qwen3.5/

*본 문서는 2026‑02‑19 현재 Hugging Face Model Card에 공개된 정보를 기반으로 작성되었습니다.*
