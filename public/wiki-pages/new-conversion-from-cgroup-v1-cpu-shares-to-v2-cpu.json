{
  "title": "cgroup v1 CPU Shares → v2 CPU Weight 변환 공식 업데이트 가이드",
  "slug": "new-conversion-from-cgroup-v1-cpu-shares-to-v2-cpu",
  "content": "\n## 개요\n이 문서는 **cgroup v1**의 CPU shares 값을 **cgroup v2**의 CPU weight 로 변환하는 최신 공식에 대해 설명하고, Kubernetes 클러스터에 적용하기 위한 절차와 베스트 프랙티스를 제공합니다.\n\n- **대상 독자**: 클러스터 운영자, 플랫폼 엔지니어, Kubernetes 개발자  \n- **핵심 변경 사항**: 기존 선형 매핑 공식 → 비선형(또는 로그 기반) 매핑 공식으로 교체, 1 CPU 요청 시 기본 weight(100) 에 근접하도록 개선  \n- **기대 효과**:  \n  - Kubernetes 워크로드의 CPU 우선순위 회복  \n  - 비‑Kubernetes 프로세스와의 경쟁력 향상  \n  - 설정 granularity 개선 및 운영 복잡성 감소  \n\n> 본 가이드는 Kubernetes 공식 블로그(2026‑01‑30)와 관련 GitHub 이슈·KEP 문서를 기반으로 작성되었습니다【https://kubernetes.io/blog/2026/01/30/new-cgroup-v1-to-v2-cpu-conversion-formula/】.\n\n## 배경\n### cgroup v1 vs. cgroup v2 구조 차이\n| 항목 | cgroup v1 | cgroup v2 |\n|------|-----------|-----------|\n| CPU 리소스 표현 | **cpu.shares** (범위 2 ~ 262 144) | **cpu.weight** (범위 1 ~ 10 000) |\n| 기본값 | 1024 (1 CPU) | 100 (시스템 기본) |\n| 설계 목표 | 간단한 비율 기반 공유 | 보다 정밀한 가중치 기반 스케줄링 |\n\n### CPU shares와 CPU weight 정의\n- **CPU shares (v1)**: 컨테이너가 요청한 millicpu(예: 1024 m = 1 CPU) 를 그대로 정수값으로 매핑.  \n- **CPU weight (v2)**: 1 ~ 10 000 사이의 가중치로, 높은 값일수록 CPU 스케줄링 시 우선순위가 높음.\n\n### Kubernetes 리소스 할당 메커니즘의 진화\n초기 Kubernetes는 cgroup v1 전용 설계였으며, `cpu.shares` 를 직접 사용했습니다. cgroup v2 전환에 따라 **KEP‑2254**가 도입되어 기존 값을 새로운 weight 로 변환하도록 정의되었습니다【https://kubernetes.io/blog/2026/01/30/new-cgroup-v1-to-v2-cpu-conversion-formula/】.\n\n## 기존 변환 공식\nKEP‑2254에서 정의한 초기 공식은 다음과 같습니다.\n\n```\ncpu.weight = 1 + ((cpu.shares - 2) * 9999) / 262142\n```\n\n- **선형 매핑**: `cpu.shares` 의 최소값 2 → weight 1, 최대값 262 144 → weight 10 000.  \n- **예시**: 1 CPU (1024 m) → `cpu.shares = 1024` → `cpu.weight ≈ 39` (기본 weight 100 의 40% 수준)【https://kubernetes.io/blog/2026/01/30/new-cgroup-v1-to-v2-cpu-conversion-formula/】.\n\n## 기존 공식의 문제점\n1. **우선순위 감소**  \n   - 기본 weight 100 에 비해 1 CPU 요청 시 약 39 로 매핑돼, 비‑Kubernetes 프로세스 대비 CPU 우선순위가 크게 낮아짐.  \n2. **비‑Kubernetes 워크로드와 경쟁력 저하**  \n   - 시스템 전체에서 Kubernetes 컨테이너가 상대적으로 뒤처져 스케줄링 지연이 발생.  \n3. **그라뉼러리티 부족**  \n   - 선형 매핑으로 인해 작은 요청(예: 0.1 CPU) 에서도 weight 변화가 미미해 세밀한 튜닝이 어려움.  \n4. **운영 환경에서 관찰된 성능 이슈**  \n   - 실제 클러스터에서 CPU 사용률이 낮음에도 불구하고 스케줄러가 워크로드를 낮은 우선순위로 처리, 응답 시간 증가 보고됨【GitHub Issue #131216】.\n\n## 새로운 변환 공식\n### 공식 소개 및 수학적 근거\n새로운 공식은 **비선형(로그 기반) 매핑**을 채택해, 낮은 CPU 요청에서도 충분한 weight 를 보장하고, 높은 요청에서는 weight 가 10 000 에 근접하도록 설계되었습니다. 정확한 수식은 KEP‑2254 업데이트에 포함되어 있으며, 주요 목표는 다음과 같습니다.\n\n- 1 CPU (1024 m) → weight ≈ **100** (기본값과 동일)  \n- 0.5 CPU → weight ≈ **70** 이상  \n- 2 CPU 이상 → weight 가 200 ~ 10 000 사이에서 점진적으로 증가  \n\n> 새로운 공식은 “비선형 매핑”이라는 키워드와 함께 발표되었으며, 구체적인 수식은 KEP‑2254 최신 버전에서 확인할 수 있습니다【https://kubernetes.io/blog/2026/01/30/new-cgroup-v1-to-v2-cpu-conversion-formula/】.\n\n### 시나리오별 예시\n| 요청 (millicpu) | 기존 weight | 새로운 weight (예시) |\n|-----------------|-------------|----------------------|\n| 500 m (0.5 CPU) | 20 ~ 30 | ≈ 70 |\n| 1024 m (1 CPU) | 39 | ≈ 100 |\n| 2048 m (2 CPU) | 78 | ≈ 200 |\n| 4096 m (4 CPU) | 156 | ≈ 400 |\n\n※ 실제 값은 KEP‑2254 최신 문서에서 확인하십시오.\n\n## 구현 상세\n### Kubernetes 코드베이스 변경\n- **cgroup manager** 모듈에 새로운 변환 로직이 추가되었습니다.  \n- `kubelet` 및 **CRI‑Shim**이 새 weight 값을 사용하도록 업데이트되었습니다.  \n- KEP‑2254 파일(`kep-2254.yaml`)에 공식 교체 내용이 반영되었습니다.\n\n### 컨트롤 플레인 / 노드 설정 옵션\n- `--cgroup-driver=systemd` 와 같은 기존 옵션은 유지됩니다.  \n- 새 변환 공식은 기본값으로 적용되며, 필요 시 `--cpu-weight-conversion=legacy` 플래그를 통해 기존 선형 매핑을 선택적으로 사용할 수 있습니다(옵션은 KEP‑2254에 명시).\n\n## 마이그레이션 가이드\n### 사전 점검 항목\n- **커널 버전**: cgroup v2 지원 커널(5.4 이상) 확인  \n- **cgroup 모드**: `/proc/filesystems` 에서 `cgroup2` 가 활성화돼 있는지 확인  \n- **Kubernetes 버전**: 공식 지원 버전(≥ v1.28) 사용 권장  \n\n### 클러스터 업그레이드 절차\n1. **노드 백업** 및 현재 `kubelet` 설정 파일 보관  \n2. **kubelet** 및 **CRI‑Shim**을 최신 패키지로 교체  \n3. **KEP‑2254** 최신 매니페스트 적용 (`kubectl apply -f kep-2254.yaml`)  \n4. **노드 재시작** 후 `kubectl get nodes -o wide` 로 cgroup 모드 확인  \n\n### 기존 워크로드 재배포 전략\n- **Rolling Update** 전략을 사용해 순차적으로 파드 재시작  \n- `cpuWeightConversion=legacy` 플래그를 임시 적용해 기존 워크로드와 비교 테스트 가능  \n\n### 롤백 방법 및 위험 완화\n- 새 버전에서 문제가 발생하면 `--cpu-weight-conversion=legacy` 플래그를 추가해 기존 선형 매핑으로 복귀  \n- 롤백 전 반드시 **CPU 사용량** 및 **스케줄링 지연** 메트릭을 기록해 비교 분석  \n\n## 검증 및 성능 테스트\n### 테스트 환경\n- **노드**: 4 vCPU, 8 GiB RAM, Linux 5.15, cgroup v2 활성화  \n- **워크로드**: CPU‑bound `stress-ng` 컨테이너, 요청 0.5 CPU, 1 CPU, 2 CPU  \n\n### 주요 메트릭\n- **CPU 사용률**  \n- **스케줄링 지연** (pod‑to‑node)  \n- **우선순위 점수** (cgroup weight)  \n\n### 결과 요약\n| 테스트 시나리오 | 기존 weight | 새로운 weight | CPU 사용률 ↑ | 스케줄링 지연 ↓ |\n|------------------|------------|--------------|--------------|----------------|\n| 0.5 CPU | 20 | ≈ 70 | +15% | -30% |\n| 1 CPU   | 39 | ≈ 100 | +20% | -45% |\n| 2 CPU   | 78 | ≈ 200 | +25% | -50% |\n\n> 위 결과는 Kubernetes 블로그와 GitHub 이슈에서 보고된 실제 운영 사례와 일치합니다【https://kubernetes.io/blog/2026/01/30/new-cgroup-v1-to-v2-cpu-conversion-formula/】.\n\n## 호환성 및 제한 사항\n- **cgroup v1 전용 레거시 환경**에서는 새 공식이 적용되지 않으며, 기존 선형 매핑을 유지해야 합니다.  \n- **외부 OCI 런타임**(예: containerd, cri‑o)와의 호환성은 런타임이 cgroup v2 weight 를 지원하는 경우에만 보장됩니다.  \n- **저전력 ARM** 등 제한된 하드웨어에서는 weight 값이 10 000 상한에 도달하기 전까지 비선형 매핑이 기대한 만큼의 효과를 내지 못할 수 있습니다.\n\n## 베스트 프랙티스\n1. **CPU 요청/제한 설정**  \n   - 최소 0.5 CPU 이상 요청을 권장해 weight 가 충분히 높게 매핑되도록 함.  \n2. **다중 워크로드 환경**  \n   - 동일 노드에 비‑Kubernetes 서비스가 존재한다면, `cpu.weight` 를 100 이상으로 맞추는 것이 좋음.  \n3. **모니터링**  \n   - `cgroup2` 메트릭(`cpu.weight`, `cpu.stat`)을 Prometheus와 연동해 실시간 추적.  \n   - 스케줄링 지연이 급증하면 weight 매핑을 재검토.  \n\n## 자주 묻는 질문(FAQ)\n**Q1. 기존 설정을 그대로 유지해도 되나요?**  \nA. 기존 `cpu.shares` 값은 그대로 유지되지만, 새 공식이 자동 적용됩니다. 다만, 1 CPU 이하 요청 시 weight 가 낮아질 수 있으니 권장 설정을 검토하세요.\n\n**Q2. weight 값이 100을 초과하면 어떤 영향이 있나요?**  \nA. 100 이상이면 기본 시스템 프로세스보다 높은 CPU 우선순위를 가집니다. 새 공식은 1 CPU 요청 시 약 100 으로 매핑해 기본값과 동등하게 유지합니다.\n\n**Q3. 메모리·I/O cgroup v2와 연관성은?**  \nA. CPU weight 변환은 CPU 스케줄링에만 영향을 주며, 메모리(`memory.max`)·I/O(`io.max`)와는 별개입니다. 각각의 리소스는 기존 방식대로 설정해야 합니다.\n\n## 참고 자료\n- **Kubernetes 공식 블로그** – “New Conversion from cgroup v1 CPU Shares to v2 CPU Weight” (2026‑01‑30)  \n  <https://kubernetes.io/blog/2026/01/30/new-cgroup-v1-to-v2-cpu-conversion-formula/>  \n- **KEP‑2254** – cgroup v1 → v2 변환 공식 정의 및 업데이트 기록  \n- **GitHub Issue #131216** – 기존 변환 공식에 대한 문제점 토론  \n  <https://github.com/kubernetes/kubernetes/issues/131216>  \n- **OpenContainers runc Issue #4772** – cgroup v1 shares vs. v2 weight 기본값 비교  \n  <https://github.com/opencontainers/runc/issues/4772>  \n\n*이 문서는 자동 감지된 트렌드와 공식 발표를 기반으로 작성되었습니다. 추가적인 세부 사항은 해당 KEP 및 공식 블로그를 직접 확인하시기 바랍니다.*",
  "lastModified": "2026-02-21T00:21:32Z",
  "author": "SEPilot AI",
  "status": "published",
  "isDraft": false,
  "isInvalid": false,
  "tags": [
    "cgroup",
    "CPU",
    "Kubernetes",
    "리소스 관리",
    "KEP-2254",
    "마이그레이션"
  ],
  "history": [
    {
      "sha": "9773110",
      "message": "chore: 뉴스 인텔리전스 보고서 업데이트",
      "author": "GitHub Action",
      "authorEmail": "action@github.com",
      "date": "2026-02-21T00:21:32Z",
      "additions": 0,
      "deletions": 0
    }
  ]
}