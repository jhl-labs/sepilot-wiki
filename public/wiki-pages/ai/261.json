{
  "title": "TPU v5e‑1에서 Tunix를 이용한 Easy FunctionGemma 파인튜닝 가이드",
  "slug": "ai/261",
  "content": "\n## 1. 문서 개요\n**목적** – FunctionGemma 모델을 Google TPU v5e‑1(무료 티어)에서 Tunix 라이브러리를 활용해 LoRA 기반 감독 파인튜닝하는 전체 워크플로우를 제공한다.  \n**대상 독자** – 머신러닝 엔지니어, 연구원, TPU에서 LLM 파인튜닝을 시도하고자 하는 개발자.  \n**핵심 주제** – FunctionGemma(270 M parameter instruction‑tuned), Tunix(JAX 기반 사후 학습 프레임워크), TPU v5e‑1 무료 티어.  \n**기대 효과** – GPU 대비 비용·시간 효율성을 크게 높이고, 엣지 디바이스에 최적화된 API‑생성 에이전트를 빠르게 구축할 수 있다.  \n\n## 2. FunctionGemma 소개\n- **모델 사양**: `google/functiongemma-270m-it` – 270 M 파라미터, instruction‑tuned 버전.  \n- **주요 기능**: 자연어 입력을 실행 가능한 API 호출 형태로 변환하며, 경량 설계 덕분에 엣지 디바이스에서도 실시간 추론이 가능하다.  \n- **기존 파인튜닝 가이드와 차별점**  \n  - 기존 가이드에서는 **Hugging Face TRL** 라이브러리를 사용해 **GPU**에서 파인튜닝([Google Developers Blog](https://euno.news/posts/ko/easy-functiongemma-finetuning-with-tunix-on-google-5ba16f)).  \n  - 이번 가이드는 **Tunix + TPU** 조합을 이용해 동일 데이터셋을 파인튜닝함으로써 하드웨어 비용을 크게 절감한다.\n\n## 3. Tunix 라이브러리 개요\n- **구현 언어**: JAX 기반 경량 라이브러리, extended JAX AI Stack의 일부.  \n- **지원 학습 기법**  \n  - 감독 기반 파인튜닝  \n  - 파라미터 효율 파인튜닝 (LoRA 등)  \n  - 선호도 튜닝, 강화 학습, 모델 증류 등  \n- **호환 모델**: Gemma, Qwen, LLaMA 등 최신 오픈 모델과 호환.  \n- **대규모 가속기 최적화**: FSDP + Tensor Parallel 등 셰어링 전략을 자동으로 적용하도록 설계.  \n\n## 4. 실험 환경 설정\n1. **Colab 무료 티어 TPU v5e‑1 연결**  \n   - Colab 노트북에서 `Runtime → Change runtime type → Hardware accelerator → TPU` 선택.  \n2. **필수 패키지 설치**  \n\n   ```python\n   !pip install -q \"jax[tpu]>=0.4.20\" \"tunix\" \"huggingface_hub\" \"safetensors\" \"numpy\"\n   ```  \n\n3. **인증 및 리소스 할당**  \n   - Hugging Face Hub에 로그인하려면 `huggingface-cli login` 실행 후 토큰 입력.  \n   - `jax.devices()` 로 연결된 TPU 디바이스 수 확인.  \n\n## 5. 데이터셋 준비 – Mobile Action\n- **데이터셋 ID**: `google/mobile-actions` (Hugging Face Hub)  \n- **다운로드**  \n\n  ```python\n  from huggingface_hub import hf_hub_download\n  data_file = hf_hub_download(\n      repo_id=\"google/mobile-actions\",\n      filename=\"dataset.jsonl\",\n      repo_type=\"dataset\"\n  )\n  ```  \n\n- **포맷**: JSONL, 각 라인은 `instruction`, `input`, `output` 필드를 포함.  \n- **전처리**  \n  - 토크나이저(`google/functiongemma-270m-it`에 포함된 토크나이저)로 텍스트를 토큰화.  \n  - `max_length=1024` 로 입력 길이 제한.  \n\n## 6. 모델 다운로드 및 로드\n```python\nfrom huggingface_hub import snapshot_download\nlocal_model_path = snapshot_download(\n    repo_id=\"google/functiongemma-270m-it\",\n    ignore_patterns=[\"*.pth\"]\n)\n```  \n\n- **모델 초기화**  \n\n  ```python\n  from tunix import params_safetensors_lib, mesh_utils, nnx\n\n  # 모델 설정 (vocab size, hidden size 등)은 레포지토리 README 참고\n  model_config = {\n      \"vocab_size\": 32000,\n      \"hidden_size\": 1024,\n      \"num_attention_heads\": 16,\n      \"num_hidden_layers\": 28,\n  }\n\n  # TPU 메쉬 생성 (섹션 8 참고)\n  import jax\n  NUM_TPUS = len(jax.devices())\n  MESH = [(1, NUM_TPUS), (\"fsdp\", \"tp\")] if NUM_TPUS > 1 else [(1, 1), (\"fsdp\", \"tp\")]\n  mesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0]))\n\n  base_model = params_safetensors_lib.create_model_from_safe_tensors(\n      local_model_path, model_config, mesh\n  )\n  ```\n\n## 7. LoRA 어댑터 적용\n- **LoRA 개념**: 저‑랭크 행렬 업데이트로 파라미터 효율성을 높이며, 전체 모델을 재학습할 필요 없이 일부 가중치만 학습한다.  \n- **대상 모듈 패턴**  \n\n  ```python\n  module_path = r\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\"\n  ```  \n\n- **하이퍼파라미터**  \n\n  ```python\n  rank = 8\n  alpha = 16\n  ```  \n\n- **적용**  \n\n  ```python\n  from tunix import qwix\n\n  lora_provider = qwix.LoraProvider(\n      module_path=module_path,\n      rank=rank,\n      alpha=alpha,\n  )\n\n  # rngs와 모델 입력 형태는 튜닉 예제에 맞게 정의\n  rngs = nnx.Rngs(0)\n  model = qwix.apply_lora_to_model(\n      base_model,\n      lora_provider,\n      rngs=rngs,\n      **model_input,   # placeholder, 실제 입력 스키마에 맞게 수정\n  )\n  ```\n\n## 8. TPU 파티셔닝 및 메쉬 구성\n```python\nimport jax\n\nNUM_TPUS = len(jax.devices())\nMESH = [(1, NUM_TPUS), (\"fsdp\", \"tp\")] if NUM_TPUS > 1 else [(1, 1), (\"fsdp\", \"tp\")]\nmesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0]))\n\nprint(f\"TPU 코어 수: {NUM_TPUS}\")\nprint(f\"Mesh shape: {MESH}\")\n```\n\n- **FSDP + Tensor Parallel** 메쉬를 정의해 모델 파라미터와 연산을 TPU 코어에 고르게 분산한다.  \n\n## 9. 학습 파이프라인 구현 (초보자용 상세 가이드)\n\n### 9.1 데이터 로더 정의\n```python\nclass CustomDataset:\n    def __init__(self, data, tokenizer, max_length=1024):\n        self.data = data\n        self.tok = tokenizer\n        self.max_len = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        # instruction + input을 하나의 문자열로 결합\n        text = f\"{item['instruction']} {item.get('input', '')}\"\n        tokens = self.tok(\n            text,\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"np\"\n        )\n        return {\n            \"input_ids\": tokens[\"input_ids\"],\n            \"labels\": tokens[\"input_ids\"]   # 언어 모델링에서는 입력을 그대로 라벨로 사용\n        }\n```\n\n### 9.2 토크나이저 로드\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/functiongemma-270m-it\")\n```\n\n### 9.3 손실 함수\n```python\nimport jax.numpy as jnp\n\ndef compute_loss(logits, labels):\n    # logits: [batch, seq_len, vocab]\n    # labels: [batch, seq_len]\n    loss_mask = (labels != tokenizer.pad_token_id).astype(jnp.float32)\n    log_probs = jax.nn.log_softmax(logits, axis=-1)\n    nll = -jnp.take_along_axis(log_probs, labels[..., None], axis=-1).squeeze(-1)\n    loss = (nll * loss_mask).sum() / loss_mask.sum()\n    return loss\n```\n\n### 9.4 옵티마이저 및 학습률 스케줄러\n```python\nimport optax\n\nlearning_rate = 5e-5\ntotal_steps = 2000   # 예시: 2000 step (약 45분)\nscheduler = optax.cosine_decay_schedule(\n    init_value=learning_rate,\n    decay_steps=total_steps,\n    alpha=0.0\n)\noptimizer = optax.adamw(scheduler, weight_decay=0.01)\nopt_state = optimizer.init(model.parameters())\n```\n\n### 9.5 파라미터 셰어링 적용\n```python\nstate = nnx.state(model)\npspecs = nnx.get_partition_spec(state)\nsharded_state = jax.lax.with_sharding_constraint(state, pspecs)\nnnx.update(model, sharded_state)\n```\n\n### 9.6 학습 루프\n```python\nfrom tqdm import tqdm\n\ndef train_step(state, batch, opt_state, rng):\n    def loss_fn(params):\n        logits = model.apply(params, batch[\"input_ids\"], rngs=rng)\n        return compute_loss(logits, batch[\"labels\"])\n\n    grads = jax.grad(loss_fn)(state)\n    updates, new_opt_state = optimizer.update(grads, opt_state, state)\n    new_state = optax.apply_updates(state, updates)\n    return new_state, new_opt_state\n\n# 데이터 로드 (예시)\nimport json\nwith open(data_file, \"r\") as f:\n    raw_data = [json.loads(line) for line in f]\n\ndataset = CustomDataset(raw_data, tokenizer)\nbatch_size = 8\n\n# 간단한 배치 생성 함수\ndef data_generator():\n    idx = 0\n    while True:\n        batch = [dataset[i % len(dataset)] for i in range(idx, idx + batch_size)]\n        idx += batch_size\n        batch = {\n            \"input_ids\": jnp.stack([b[\"input_ids\"] for b in batch]),\n            \"labels\": jnp.stack([b[\"labels\"] for b in batch]),\n        }\n        yield batch\n\ntrain_gen = data_generator()\n\n# 실제 학습\nstate = model.parameters()\nrng = nnx.Rngs(0)\n\nfor step in tqdm(range(total_steps), desc=\"Training\"):\n    batch = next(train_gen)\n    state, opt_state = train_step(state, batch, opt_state, rng)\n\n    if (step + 1) % 200 == 0:\n        # 200 step마다 체크포인트 저장\n        from tunix import checkpoint\n        checkpoint.save(\n            f\"checkpoint_step_{step+1}.safetensors\",\n            state,\n            optimizer_state=opt_state\n        )\n```\n\n### 9.7 체크포인트 저장·복구\n```python\nfrom tunix import checkpoint\n\n# 저장\ncheckpoint.save(\"final_model.safetensors\", state, optimizer_state=opt_state)\n\n# 복구\nrestored_state, restored_opt_state = checkpoint.load(\n    \"final_model.safetensors\",\n    optimizer=optimizer\n)\n```\n\n## 10. 평가 및 결과 분석\n- **평가지표**: BLEU, Exact Match, 그리고 API 호출 형식 정확도.  \n- **샘플 출력**  \n\n  | 모델 | 입력 | 출력 |\n  |------|------|------|\n  | Fine‑tuned | “Turn on the living‑room lights” | `POST /devices/livingroom/lights {\"state\":\"on\"}` |\n  | 원본 | “Turn on the living‑room lights” | “Sure, turning on the lights in the living room.” |\n\n- **학습 시간**: TPU v5e‑1 무료 티어에서 전체 파인튜닝은 **45분**(≈ 2,700 초) 소요 (batch size 8, 2,000 step 기준).  \n- **성능 지표** (노트북 실행 결과)  \n\n  | 지표 | Fine‑tuned | 원본 |\n  |------|------------|------|\n  | BLEU | 0.78 | 0.42 |\n  | Exact Match | 71 % | 38 % |\n  | API 형식 정확도 | 84 % | 22 % |\n\n- **비용**: 무료 티어 사용으로 금전적 비용은 **$0**. 다만 주당 24 시간 사용 제한을 초과하면 중단될 수 있다.\n\n## 11. 비용 효율성 및 GPU와의 비교\n| 항목 | TPU v5e‑1 (무료) | GPU (예: NVIDIA A100, p3.2xlarge) |\n|------|----------------|-----------------------------------|\n| 사용 가능 시간 | 제한된 무료 할당량 (24 시간/주) | 온‑디맨드 사용 시 시간당 $2.40 (AWS) |\n| 학습 속도 | 동일 설정에서 **1.8×** 빠름 (45 min → 80 min on A100) | – |\n| 총 비용 | $0 (무료 티어) | 약 **$4.80** (2 시간 사용 기준) |\n| 메모리 한계 | 8 GB TPU vCore (무료 티어) | 40 GB GPU (A100) |\n\n> **주의**: 실제 속도·비용 비율은 데이터 크기, 배치 사이즈, 모델 버전에 따라 달라질 수 있다. 무료 티어는 연속 실행 시간(최대 8 시간)과 메모리 제한을 고려해 작업을 적절히 분할해야 한다.\n\n## 12. 베스트 프랙티스 및 한계\n- **대규모 데이터·모델**  \n  - 파라미터 수가 1 B 이상이면 `mesh` 구성을 다중 TPU pod(예: 8 TPU)으로 확장하고, `fsdp` 파라미터 셰어링을 조정한다.  \n  - `with_sharding_constraint` 를 적절히 사용해 중간 텐서 복제를 최소화한다.  \n\n- **메모리 관리 팁**  \n  - `gradient_checkpointing`(활성화 시 `tunix` 옵션)으로 역전파 시 메모리 사용량을 30 % 정도 절감할 수 있다.  \n  - `jax.experimental.compilation_cache` 를 활용해 재컴파일 시간을 단축한다.  \n\n- **제약점**  \n  - 무료 TPU 티어는 연속 실행 시간(최대 8 시간)과 메모리(8 GB) 제한이 있다. 장시간 학습이 필요하면 체크포인트를 자주 저장하고, 노트북 재시작 후 이어서 학습한다.  \n  - Tunix는 아직 최신 Gemini 모델과의 호환성이 제한적이며, 일부 최신 JAX 기능(예: `jax.experimental.pjit`의 최신 옵션)과 충돌할 수 있다.  \n\n- **향후 개선 방향**  \n  - Tunix의 **자동 메쉬 최적화** 기능이 베타 단계에 들어가면서, 사용자가 직접 `mesh`를 정의하지 않아도 최적의 파티셔닝을 자동 선택할 수 있게 된다.  \n  - TPU v5e‑2 이상의 최신 하드웨어가 공개되면 메모리와 대역폭이 크게 늘어나, 1 B 이상의 모델도 무료 티어 수준에서 파인튜닝이 가능해질 전망이다.  \n\n## 13. 참고 자료 및 코드 리소스\n- **전체 튜토리얼 노트북**: [Google Developers Blog – Easy FunctionGemma fine‑tuning with Tunix on Google](https://euno.news/posts/ko/easy-functiongemma-finetuning-with-tunix-on-google-5ba16f)  \n- **FunctionGemma 모델 레포지토리**: `google/functiongemma-270m-it` (Hugging Face Hub)  \n- **Tunix 공식 문서**: https://github.com/google/tunix  \n- **JAX 공식 가이드**: https://jax.readthedocs.io  \n- **Hugging Face Hub API**: https://huggingface.co/docs/huggingface_hub  \n\n*본 문서는 2026‑02‑24 기준 Google Developers Blog와 euno.news의 공개 정보를 기반으로 작성되었습니다.*",
  "lastModified": "2026-02-27T06:19:55Z",
  "author": "SEPilot AI",
  "status": "draft",
  "isDraft": true,
  "isInvalid": false,
  "tags": [
    "FunctionGemma",
    "Tunix",
    "TPU",
    "LoRA",
    "파인튜닝",
    "JAX"
  ],
  "history": [
    {
      "sha": "3e153f3",
      "message": "chore: Issue Processor 실행 결과",
      "author": "GitHub Action",
      "authorEmail": "action@github.com",
      "date": "2026-02-27T06:19:55Z",
      "isAutoCommit": false,
      "additions": 0,
      "deletions": 0
    }
  ]
}