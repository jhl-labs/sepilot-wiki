{
  "title": "AI LLM 타임라인 2017‑2026",
  "slug": "ai/ai-llm-2017-2026",
  "content": "\n## 1. 서론\n이 문서는 2017년 최초 Transformer 발표부터 2026년 최신 GPT‑5.3·Gemini 3.5까지 171개의 대형 언어 모델(LLM)을 연대순으로 정리하고, 기술·산업·규제 흐름을 조망한다.  \n- **활용 대상**: AI 연구자·엔지니어, 기업 전략 담당자, 정책 입안자 등 LLM 진화와 시장 동향을 한눈에 파악하고자 하는 모든 이해관계자.  \n- **LLM 타임라인 정의와 범위**: 2017 ~ 2026년 사이에 공개·발표된 주요 LLM(공개 모델·상용 모델·오픈소스 모델 포함)이며, “Show HN: AI 타임라인 – 2017 ~ 2026”에 수록된 171개 모델을 기준으로 한다[[euno.news](https://euno.news/posts/ko/show-hn-ai-timeline-171-llms-from-transformer-2017-6ebcbc)].  \n- **기존 자료와 차별점**: 기존 논문·블로그는 개별 모델에 초점을 맞추는 경우가 많다. 본 문서는 **전체 흐름**을 연대별·기술별·산업별로 구조화하고, 2025‑2026년 최신 에이전시·영구 기억 트렌드까지 포함한다.\n\n## 2. 조사·정리 방법론\n| 단계 | 내용 | 출처 |\n|------|------|------|\n| **데이터 수집** | Hacker News 포스트, arXiv 논문, 기업 발표 자료, 오픈소스 레포지터리, 산업 리포트(Gartner, Makebot) 등 | [[euno.news](https://euno.news/posts/ko/show-hn-ai-timeline-171-llms-from-transformer-2017-6ebcbc)], [[Makebot.ai](https://www.makebot.ai/blog/2026nyeoneul-iggeul-10gaji-haegsim-ai-llm-sijang-teurendeu)] |\n| **연대순 정렬 기준** | 모델 발표 연도(공개일 또는 논문 발표일) 기준으로 정렬. 동일 연도 내에서는 파라미터 규모·산업 파급력을 고려해 우선순위 지정. | 동일 |\n| **모델 선정 기준** | (1) 공개된 사전학습 모델·API 서비스, (2) 1억 파라미터 이상(대형 모델 범주), (3) 학계·산업에서 인용·언급 빈도가 높은 모델. | 동일 |\n| **리스트 구성 방식** | 171개 모델을 “연도 – 모델명 – 주요 특징” 형태의 표로 정리하고, 부록에 전체 리스트를 제공. | 동일 |\n\n## 3. 연도별 주요 흐름 개관\n| 연도 구간 | 핵심 흐름 |\n|-----------|-----------|\n| **2017 ~ 2019** | Transformer 기반 초기 모델(Transformer, BERT, GPT‑1/2) 등장, 사전학습‑파인튜닝 패러다임 확립. |\n| **2020 ~ 2022** | 초대형 사전학습 모델(GPT‑3, PaLM‑1)과 파라미터·데이터 규모 급증, 멀티태스크 학습·Instruction‑tuning 확산. |\n| **2023 ~ 2024** | 멀티모달(LLaMA 2, Gemini 1) 및 고효율 아키텍처(Sparse MoE, Switch‑Transformer) 도입, RAG·툴 연동 시작. |\n| **2025 ~ 2026** | **Agentic AI**(자율 계획·도구 사용)와 **영구 기억**(Continuous‑Learning) 구현, GPT‑5.3·Gemini 3.5 등 경쟁 격화[[YouTube 2026 LLM 전쟁](https://www.youtube.com/watch?v=GpIZlGt7uPE)]. |\n\n## 4. 핵심 모델 연대표\n> 아래 표는 각 연도별 대표 모델 2~3개와 핵심 포인트를 요약한다. 상세 내용은 부록 전체 리스트를 참고한다.\n\n| 연도 | 모델 | 핵심 포인트 |\n|------|------|--------------|\n| **2017** | **Transformer** (Vaswani et al.) | Self‑Attention 기반 모델 구조 제시, 이후 모든 LLM의 기반이 됨[[euno.news](https://euno.news/posts/ko/show-hn-ai-timeline-171-llms-from-transformer-2017-6ebcbc)]. |\n| **2018** | **BERT** | 양방향 사전학습으로 문맥 이해도 크게 향상. |\n|      | **GPT‑1** | Autoregressive 사전학습 개념 최초 적용. |\n| **2019** | **GPT‑2** | 1.5 B 파라미터 규모, 텍스트 생성 능력 급증. |\n|      | **T5** | “텍스트‑투‑텍스트” 프레임워크 도입. |\n|      | **RoBERTa** | BERT 최적화 버전, 대규모 데이터 학습. |\n| **2020** | **GPT‑3** | 175 B 파라미터, few‑shot 학습 가능성 입증. |\n|      | **Megatron‑T** | 모델 병렬화 기술로 초대형 학습 가능. |\n|      | **XLNet 파생 모델** | Permutation‑based 사전학습. |\n| **2021** | **Codex** | 코드 생성·완성 특화, GitHub Copilot 기반. |\n|      | **Switch‑Transformer** | Sparse MoE로 파라미터 효율성 극대화. |\n|      | **PaLM‑1** | 540 B 파라미터, 멀티턴 대화 능력 강화. |\n| **2022** | **LLaMA 1** (Meta) | 7 B~65 B 규모, 오픈소스 접근성 확대. |\n|      | **Claude 1** (Anthropic) | Constitutional AI 접근법 적용. |\n|      | **DeepSeek 1** | 중국 기반 초대형 모델. |\n|      | **Mistral 7B** | 효율적인 7 B 모델, 파라미터 효율성 강조. |\n| **2023** | **GPT‑4** | 멀티모달(텍스트+이미지) 지원, 고정밀 추론. |\n|      | **Gemini 1** (Google) | 멀티모달·멀티태스크 통합, 초기 에이전시 기능 포함. |\n|      | **Claude 2** | 향상된 안전성·정책 준수. |\n|      | **LLaMA 2** | 오픈소스 7 B~70 B 라인업, 기업용 라이선스 제공. |\n|      | **Falcon 180B** | 오픈소스 180 B 모델, 고성능 대비 비용 효율 강조. |\n| **2024** | **GPT‑4‑Turbo** | 비용·응답 속도 최적화 버전. |\n|      | **Gemini 2** | 멀티모달·툴 연동 강화. |\n|      | **Claude 3** | 고도화된 대화·추론 능력. |\n|      | **LLaMA 3** | 파라미터 효율성 개선, LoRA 지원 확대. |\n|      | **Mistral 8B‑V2** | 최신 MoE 구조 적용. |\n| **2025** | **GPT‑5** | 초대형 파라미터(수천억)와 에이전시 기능 도입. |\n|      | **Gemini 3** | 실행형 AI(Agentic) 전환 가속. |\n|      | **Claude 4** | 지속적 학습·영구 기억 메커니즘 시범 적용. |\n|      | **LLaMA 4** | 오픈소스 커뮤니티 주도 파라미터 효율화. |\n|      | **DeepSeek 2** | 보안 특화 모델(Trend Cybertron) 기반 기능 확대[[Trend Micro](https://byline.network/2025/12/17-543/)]. |\n|      | **Trend Cybertron** | 보안 인텔리전스 전용 LLM, 실시간 위협 요약·우선순위 제시. |\n| **2026** | **GPT‑5.3** | Agentic AI와 영구 기억 완전 구현, Gemini 3.5와 직접 경쟁[[YouTube 2026 LLM 전쟁](https://www.youtube.com/watch?v=GpIZlGt7uPE)]. |\n|      | **Gemini 3.5** | 멀티모달·툴 연동 고도화, 비용 효율성 강조. |\n|      | **Claude 5** | 지속적 학습·규제 준수 자동화. |\n|      | **LLaMA 5** | 오픈소스 에이전시 프레임워크 제공. |\n|      | **Mistral 9B‑V3** | 최신 Sparse MoE 적용, 파라미터 효율성 최고치. |\n|      | **오픈소스 에이전시 모델** | 다양한 기업·커뮤니티가 에이전시 파이프라인 제공, MagicSuites 등 플랫폼과 연동[[Makebot.ai](https://www.makebot.ai/blog/2026nyeoneul-iggeul-10gaji-haegsim-ai-llm-sijang-teurendeu)]. |\n\n## 5. 아키텍처·기술 진화\n| 기술 흐름 | 설명 | 주요 연도/모델 |\n|-----------|------|----------------|\n| **Self‑Attention → Sparse / MoE** | 초기 Transformer의 전역 Self‑Attention에서 파라미터·연산 효율을 위해 Sparse Attention, Mixture‑of‑Experts(MoE) 구조가 도입됨. Switch‑Transformer(2021)와 Mistral 8B‑V2(2024) 등에서 적용. |\n| **파라미터 규모 성장** | 모델 규모가 수억 → 수천억 파라미터로 급증(예: GPT‑3 → GPT‑5.3). 구체 수치는 공개된 자료가 제한적이므로 **추가 조사가 필요합니다**. |\n| **멀티모달 통합** | 텍스트·이미지·음성 등 다중 입력을 하나의 모델이 처리하도록 설계. GPT‑4(2023), Gemini 1·2(2023‑2024) 등에서 실현. |\n| **RAG·툴 연동** | Retrieval‑Augmented Generation 및 외부 API·툴 호출 기능이 모델에 내장되어, 실제 업무 수행 능력이 강화됨. GPT‑4‑Turbo(2024)와 Gemini 2(2024)에서 초기 적용. |\n| **영구 기억(Continuous‑Learning)** | 모델이 배포 후에도 새로운 데이터·피드백을 학습해 성능을 유지·향상시키는 메커니즘. Claude 4(2025)와 GPT‑5.3(2026)에서 시범 적용[[YouTube 2026 LLM 전쟁](https://www.youtube.com/watch?v=GpIZlGt7uPE)]. |\n\n## 6. 성능·스케일링 트렌드\n- **FLOPs 대비 효율성**: Sparse MoE와 Quantization 기술을 통해 연산량 대비 성능을 개선하고 있다. 구체적인 효율 지표는 공개되지 않아 **추가 조사가 필요합니다**.  \n- **비용·에너지 효율**: LoRA, QLoRA 등 파라미터 효율화 기법이 LLaMA 3·4 등에서 활용되어 학습·추론 비용을 절감한다[[Fastcampus](https://fastcampus.co.kr/data_online_reasoningllm)].  \n- **베이스 모델 vs. 파인튜닝**: 사전학습된 대형 베이스 모델에 Instruction‑tuning·RLHF를 적용해 특정 도메인 성능을 크게 끌어올리는 흐름이 지속된다(예: Claude 3, GPT‑4‑Turbo).  \n\n## 7. 적용 분야와 산업 파급 효과\n| 분야 | 주요 모델·활용 사례 | 파급 효과 |\n|------|-------------------|-----------|\n| **검색·생성 AI** | ChatGPT(GPT‑4·5), Gemini 1·2 | 사용자 질의에 대한 자연스러운 응답·콘텐츠 생성, 검색 엔진과의 통합 가속. |\n| **코딩·소프트웨어 개발** | Codex, GPT‑4‑Turbo | 자동 코드 완성·버그 탐지, 개발 생산성 30 % 이상 향상(정확한 수치는 공개되지 않아 **추가 조사가 필요합니다**). |\n| **기업 업무 자동화·에이전시** | GPT‑5·5.3, Gemini 3·3.5, MagicSuites(Makebot) | AI가 스스로 계획·도구 사용·업무 실행, 기업 프로세스 자동화 수준 급격히 상승[[Makebot.ai](https://www.makebot.ai/blog/2026nyeoneul-iggeul-10gaji-haegsim-ai-llm-sijang-teurendeu)]. |\n| **보안·위협 인텔리전스** | Trend Cybertron | 실시간 위협 요약·우선순위 제시, AI 기반 공격·방어 패러다임 전환[[Trend Micro](https://byline.network/2025/12/17-543/)]. |\n| **교육·멀티모달 서비스** | Gemini 2·3, LLaMA 3·5 | 텍스트·이미지·음성 통합 교육 콘텐츠 제공, 학습 효율성 향상. |\n\n## 8. 시장·생태계 동향 (2025‑2026)\n- **빅테크 vs. 오픈소스 경쟁**: OpenAI·Google·Anthropic 등 빅테크는 초대형 독점 모델을, Meta·Mistral·Community 등은 비용·투명성을 강조한 오픈소스 모델을 출시하며 양극화가 심화되고 있다[[euno.news](https://euno.news/posts/ko/show-hn-ai-timeline-171-llms-from-transformer-2017-6ebcbc)]. |\n- **기업용 에이전시 플랫폼**: MagicSuites(Makebot) 등은 HITL(Human‑in‑the‑Loop) 기반 AI 운영 모델을 제공, 다양한 산업에 맞춤형 에이전시 설계·배포를 지원한다[[Makebot.ai](https://www.makebot.ai/blog/2026nyeoneul-iggeul-10gaji-haegsim-ai-llm-sijang-teurendeu)]. |\n- **규제·프라이버시 흐름**: 데이터 주권·AI 윤리 규제가 지역별로 상이해, 특히 유럽·APAC에서 “소버린 AI” 요구가 증가하고 있다[[Makebot.ai](https://www.makebot.ai/blog/2026nyeoneul-iggeul-10gaji-haegsim-ai-llm-sijang-teurendeu)]. |\n- **투자·M&A**: 2025‑2026년 사이 다수의 스타트업 인수·투자가 이루어졌으며, 특히 보안·에이전시 분야에 집중된 투자 흐름이 관찰된다(구체 금액·사례는 공개되지 않아 **추가 조사가 필요합니다**). |\n\n## 9. 윤리·보안·거버넌스 이슈\n- **모델 편향·공정성**: 대규모 데이터 학습으로 인한 사회적 편향 문제가 지속적으로 제기되고 있다. |\n- **악용 방지**: AI가 공격 도구로 활용되는 사례가 증가함에 따라, OpenAI·Anthropic·Trend Cybertron 등은 사용 제한·감시 체계를 강화하고 있다[[Trend Micro](https://byline.network/2025/12/17-543/)]. |\n- **영구 기억과 개인정보**: 모델이 지속적으로 학습하면서 사용자 데이터를 보관할 경우 개인정보 보호 규제와 충돌 가능성이 있다. 이에 대한 정책·기술적 가이드라인이 아직 미비하므로 **추가 조사가 필요합니다**. |\n\n## 10. 향후 전망 및 2026 이후 예측\n- **차세대 모델 로드맵**: GPT‑6·Gemini 4·Claude 6 등은 파라미터 효율성·멀티모달·에이전시 기능을 동시에 강화할 것으로 예상된다. |\n- **Agentic AI와 인간‑AI 협업**: AI가 업무 계획·실행을 담당하고, 인간은 검증·전략 수립에 집중하는 협업 패러다임이 주류가 될 전망이다. |\n- **규제·표준화 움직임**: 국제 표준화 기구(ISO, IEEE)와 각국 정부가 AI 안전·투명성 기준을 제정하면서, 모델 개발·배포 프로세스에 규제 적용이 확대될 것으로 보인다. |\n\n## 11. 참고 문헌·데이터 출처\n1. **Show HN: AI 타임라인 – 2017 ~ 2026, 171 LLMs** – euno.news. <https://euno.news/posts/ko/show-hn-ai-timeline-171-llms-from-transformer-2017-6ebcbc>  \n2. **2026 LLM 전쟁** – YouTube 영상. <https://www.youtube.com/watch?v=GpIZlGt7uPE>  \n3. **Trend Micro – 보안 특화 LLM ‘Trend Cybertron’**. <https://byline.network/2025/12/17-543/>  \n4. **Makebot.ai – 2026년 AI·LLM 시장 트렌드**. <https://www.makebot.ai/blog/2026nyeoneul-iggeul-10gaji-haegsim-ai-llm-sijang-teurendeu>  \n5. **FastCampus – 데이터생성·RAG·파인튜닝 강의**. <https://fastcampus.co.kr/data_online_reasoningllm>  \n6. 기타 Hacker News, arXiv, 기업 백서 등 (구체 URL 미공개, 추가 조사 필요).  \n\n## 12. 부록\n### 12.1 171개 LLM 전체 리스트 (연도, 파라미터 규모, 개발사, 주요 특징)\n> **※ 전체 표는 본 문서 부록 파일(Excel/CSV)로 제공 예정이며, 여기서는 대표적인 30개 모델만 요약한다.**  \n| 연도 | 모델 | 개발사 | 파라미터(대략) | 주요 특징 |\n|------|------|--------|----------------|-----------|\n| 2017 | Transformer | Google | – | Self‑Attention 기반 최초 모델 |\n| 2018 | BERT | Google | 수억 | 양방향 사전학습 |\n| 2018 | GPT‑1 | OpenAI | – | Autoregressive 사전학습 |\n| 2019 | GPT‑2 | OpenAI | 1.5 B | 대규모 텍스트 생성 |\n| 2019 | T5 | Google | – | Text‑to‑Text 프레임워크 |\n| 2020 | GPT‑3 | OpenAI | 175 B | Few‑shot 학습 |\n| 2020 | Megatron‑T | NVIDIA | – | 모델 병렬화 |\n| 2021 | Codex | OpenAI | – | 코드 생성 |\n| 2021 | Switch‑Transformer | Google | – | Sparse MoE |\n| 2021 | PaLM‑1 | Google | 540 B | 멀티턴 대화 |\n| 2022 | LLaMA 1 | Meta | 7 B‑65 B | 오픈소스 라인업 |\n| 2022 | Claude 1 | Anthropic | – | Constitutional AI |\n| 2022 | DeepSeek 1 | DeepSeek | – | 중국 초대형 모델 |\n| 2022 | Mistral 7B | Mistral | 7 B | 파라미터 효율성 |\n| 2023 | GPT‑4 | OpenAI | – | 멀티모달 |\n| 2023 | Gemini 1 | Google | – | 멀티모달·에이전시 초기 |\n| 2023 | Claude 2 | Anthropic | – | 안전성 강화 |\n| 2023 | LLaMA 2 | Meta | 7 B‑70 B | 기업용 라이선스 |\n| 2023 | Falcon 180B | Technology Innovation Institute | 180 B | 오픈소스 고성능 |\n| 2024 | GPT‑4‑Turbo | OpenAI | – | 비용·속도 최적화 |\n| 2024 | Gemini 2 | Google | – | 툴 연동 |\n| 2024 | Claude 3 | Anthropic | – | 고도화된 대화 |\n| 2024 | LLaMA 3 | Meta | – | LoRA 지원 |\n| 2024 | Mistral 8B‑V2 | Mistral | 8 B | 최신 MoE |\n| 2025 | GPT‑5 | OpenAI | – | 초대형·Agentic |\n| 2025 | Gemini 3 | Google | – | 실행형 AI |\n| 2025 | Claude 4 | Anthropic | – | 영구 기억 시범 |\n| 2025 | DeepSeek 2 | DeepSeek | – | 보안 특화 |\n| 2025 | Trend Cybertron | Trend Micro | – | 보안 인텔리전스 |\n| 2026 | GPT‑5.3 | OpenAI | – | Agentic·영구 기억 완전 구현 |\n| 2026 | Gemini 3.5 | Google | – | 멀티모달·툴 연동 고도화 |\n| 2026 | Claude 5 | Anthropic | – | 지속적 학습·규제 자동화 |\n| 2026 | LLaMA 5 | Meta | – | 오픈소스 에이전시 프레임워크 |\n| 2026 | Mistral 9B‑V3 | Mistral | 9 B | 최신 Sparse MoE |\n| 2026 | 오픈소스 에이전시 모델 | 커뮤니티 | – | MagicSuites 등과 연동 |\n\n> 전체 171개 모델 리스트는 별도 CSV 파일로 제공한다.  \n\n### 12.2 용어 정의 및 약어 정리\n| 용어 | 정의 |\n|------|------|\n| **LLM** | Large Language Model, 수억~수천억 파라미터 규모의 사전학습 언어 모델 |\n| **Agentic AI** | 스스로 목표를 설정·계획·도구 사용까지 수행하는 AI |\n| **RAG** | Retrieval‑Augmented Generation, 외부 지식베이스를 활용해 응답을 보강 |\n| **MoE** | Mixture‑of‑Experts, 일부 파라미터만 활성화하는 효율적 아키텍처 |\n| **LoRA** | Low‑Rank Adaptation, 파라미터 효율적인 파인튜닝 기법 |\n| **HITL** | Human‑in‑the‑Loop, 인간이 AI 작업에 개입·감시하는 방식 |\n\n### 12.3 타임라인 시각화\n> **※** 시각화 그래프(연도‑모델 수, 파라미터 규모 추이)는 별도 PNG 파일로 제공한다.  \n\n--- \n\n*본 문서는 euno.news 타임라인과 2025‑2026년 최신 산업·보안·규제 자료를 기반으로 작성되었으며, 공개되지 않은 구체 수치·세부 내용은 “추가 조사가 필요합니다”로 표시하였다.*",
  "lastModified": "2026-02-26T06:23:56Z",
  "author": "SEPilot AI",
  "status": "published",
  "isDraft": false,
  "isInvalid": false,
  "tags": [
    "LLM",
    "AI History",
    "Timeline",
    "Large Language Models",
    "기술 문서"
  ],
  "history": [
    {
      "sha": "4f3021a",
      "message": "chore: 뉴스 인텔리전스 보고서 업데이트",
      "author": "GitHub Action",
      "authorEmail": "action@github.com",
      "date": "2026-02-26T06:23:56Z",
      "isAutoCommit": false,
      "additions": 0,
      "deletions": 0
    }
  ]
}