{
  "title": "Qwen 3.5",
  "slug": "ai/qwen3-5",
  "content": "\n## 1. 개요\n**Qwen 3.5**는 **Alibaba**에서 발표한 최신 대규모 언어 모델(LLM)입니다. **Gated DeltaNet + Mixture‑of‑Experts(MoE)** 아키텍처를 채택하여, 전체 397B 파라미터 중 17B만 활성화하는 방식으로 높은 성능과 효율성을 동시에 달성합니다.\n\n- **주요 목표** – 텍스트·이미지·비디오를 하나의 모델로 처리하면서, 코딩 에이전트·검색 에이전트 등 도구 활용 능력까지 갖춘 범용 AI 모델.\n- **주요 적용 분야** – 챗봇, 코딩 에이전트, 문서·이미지 분석, 다국어 번역, 의료 영상 분석 등.\n\n### 모델 사양\n| 항목 | 내용 |\n|------|------|\n| **전체 파라미터** | 397B (3,970억) |\n| **활성 파라미터** | 17B (A17B) |\n| **아키텍처** | Gated DeltaNet + MoE (512 experts, 10 routed + 1 shared) |\n| **컨텍스트 길이** | 기본 262,144 토큰, 최대 1,010,000 토큰까지 확장 |\n| **지원 언어** | 201개 언어 및 방언 |\n\n> **쉽게 말해**: MoE(Mixture‑of‑Experts)는 전문가 여러 명 중 필요한 전문가만 골라 쓰는 방식입니다. 512명의 전문가 중 매번 10명만 활성화하기 때문에, 거대한 모델이지만 실제 연산량은 17B 모델 수준으로 유지됩니다.\n\n---\n\n## 2. 모델 아키텍처\n1. **Gated DeltaNet** – 기존 Transformer의 attention 메커니즘을 개선한 구조로, 긴 문맥에서도 메모리 효율이 좋습니다.\n2. **Mixture‑of‑Experts (MoE)** – 512개의 전문가(expert) 네트워크 중 10개를 라우팅하고, 1개의 공유 전문가를 항상 활성화합니다. 이 덕분에 전체 397B 파라미터의 지식을 활용하면서도 실제 연산은 17B 수준으로 유지됩니다.\n3. **멀티모달 입력 처리** – 텍스트·이미지·비디오를 동일한 토큰 공간으로 변환하여 하나의 모델에서 처리합니다.\n4. **초장문 컨텍스트** – 기본 262K 토큰, 최대 약 100만 토큰까지 처리 가능하여 대규모 코드베이스나 긴 문서 분석에 유리합니다.\n\n---\n\n## 3. 학습 데이터 및 방법\n| 구분 | 내용 |\n|------|------|\n| **사전학습** | 다국어 텍스트, 이미지-텍스트 쌍, 코드 데이터로 멀티모달 사전학습 |\n| **후처리** | RLHF(인간 피드백 기반 강화학습)를 통한 미세조정 |\n| **지원 언어** | 201개 언어 및 방언 (다국어 벤치마크에서 최상위권 성능) |\n| **효율성 최적화** | MoE 라우팅, Mixed‑Precision(BF16) |\n\n---\n\n## 4. 주요 기능 및 특징\n| 기능 | 설명 |\n|------|------|\n| **자연어 이해·생성** | MMLU‑Pro 87.8%, SuperGPQA 70.4% 등 지식 벤치마크에서 GPT‑5.2에 근접하는 성능 |\n| **코딩 에이전트** | SWE‑bench Verified 76.4%, LiveCodeBench v6 83.6% 등 실제 코드 수정·생성 능력 검증 |\n| **멀티모달 처리** | 이미지·비디오 이해, 문서 OCR, 공간 인식 등 다양한 비전 태스크 지원 |\n| **도구·에이전트 활용** | BFCL‑V4 72.9%, MCP‑Mark 46.1% 등 도구 호출 및 에이전트 작업에서 강점 |\n| **초장문 처리** | 최대 100만 토큰 컨텍스트로 대규모 코드베이스·문서 분석 가능 |\n| **다국어 지원** | 201개 언어 지원, MMMLU 88.5%, NOVA‑63 59.1%로 다국어 벤치마크 최상위권 |\n\n---\n\n## 5. 벤치마크 성능\n\n> **출처** – [Hugging Face Model Card](https://huggingface.co/Qwen/Qwen3.5-397B-A17B). 비교 모델: GPT‑5.2, Claude 4.5 Opus, Gemini‑3 Pro, Qwen3‑Max‑Thinking, K2.5‑1T‑A32B.\n\n### 5‑1. 언어 벤치마크\n\n#### 지식 (Knowledge)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| MMLU‑Pro | **87.8** | 87.4 | 89.5 | 89.8 |\n| MMLU‑Redux | **94.9** | 95.0 | 95.6 | 95.9 |\n| SuperGPQA | **70.4** | 67.9 | 70.6 | 74.0 |\n| C‑Eval | **93.0** | 90.5 | 92.2 | 93.4 |\n\n#### 지시 수행 (Instruction Following)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| IFEval | **92.6** | 94.8 | 90.9 | 93.5 |\n| IFBench | **76.5** | 75.4 | 58.0 | 70.4 |\n| MultiChallenge | **67.6** | 57.9 | 54.2 | 64.2 |\n\n#### STEM (과학·기술·공학·수학)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| GPQA | **88.4** | 92.4 | 87.0 | 91.9 |\n| HLE | **28.7** | 35.5 | 30.8 | 37.5 |\n| HLE‑Verified | **37.6** | 43.3 | 38.8 | 48.0 |\n\n#### 추론 (Reasoning)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| LiveCodeBench v6 | **83.6** | 87.7 | 84.8 | 90.7 |\n| HMMT Feb 25 | **94.8** | 99.4 | 92.9 | 97.3 |\n| HMMT Nov 25 | **92.7** | 100 | 93.3 | 93.3 |\n| IMOAnswerBench | **80.9** | 86.3 | 84.0 | 83.3 |\n| AIME26 | **91.3** | 96.7 | 93.3 | 90.6 |\n\n#### 긴 문맥 (Long Context)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| AA‑LCR | **68.7** | 72.7 | 74.0 | 70.7 |\n| LongBench v2 | **63.2** | 54.5 | 64.4 | 68.2 |\n\n#### 일반 에이전트 (General Agent)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| BFCL‑V4 | **72.9** | 63.1 | 77.5 | 72.5 |\n| TAU2‑Bench | **86.7** | 87.1 | 91.6 | 85.4 |\n| VITA‑Bench | **49.7** | 38.2 | 56.3 | 51.6 |\n| DeepPlanning | **34.3** | 44.6 | 33.9 | 23.3 |\n| Tool Decathlon | **38.3** | 43.8 | 43.5 | 36.4 |\n| MCP‑Mark | **46.1** | 57.5 | 42.3 | 53.9 |\n\n#### 검색 에이전트 (Search Agent)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| HLE w/ tool | **48.3** | 45.5 | 43.4 | 45.8 |\n| BrowseComp | **69.0** | 65.8 | 67.8 | 59.2 |\n| BrowseComp‑zh | **70.3** | 76.1 | 62.4 | 66.8 |\n| WideSearch | **74.0** | 76.8 | 76.4 | 68.0 |\n\n#### 코딩 에이전트 (Coding Agent)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| SWE‑bench Verified | **76.4** | 80.0 | 80.9 | 76.2 |\n| SWE‑bench Multilingual | **69.3** | 72.0 | 77.5 | 65.0 |\n| SecCodeBench | **68.3** | 68.7 | 68.6 | 62.4 |\n| Terminal Bench 2 | **52.5** | 54.0 | 59.3 | 54.2 |\n\n#### 다국어 (Multilingualism)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| MMMLU | **88.5** | 89.5 | 90.1 | 90.6 |\n| MMLU‑ProX | **84.7** | 83.7 | 85.7 | 87.7 |\n| NOVA‑63 | **59.1** | 54.6 | 56.7 | 56.7 |\n| INCLUDE | **85.6** | 87.5 | 86.2 | 90.5 |\n| Global PIQA | **89.8** | 90.9 | 91.6 | 93.2 |\n| PolyMATH | **73.3** | 62.5 | 79.0 | 81.6 |\n| WMT24++ | **78.9** | 78.8 | 79.7 | 80.7 |\n| MAXIFE | **88.2** | 88.4 | 79.2 | 87.5 |\n\n### 5‑2. 비전‑언어 벤치마크\n\n#### STEM 및 퍼즐\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| MMMU | **85.0** | 86.7 | 80.7 | 87.2 |\n| MMMU‑Pro | **79.0** | 79.5 | 70.6 | 81.0 |\n| MathVision | **88.6** | 83.0 | 74.3 | 86.6 |\n| MathVista (mini) | **90.3** | 83.1 | 80.0 | 87.9 |\n| We‑Math | **87.9** | 79.0 | 70.0 | 86.9 |\n| DynaMath | **86.3** | 86.8 | 79.7 | 85.1 |\n| ZEROBench | **12** | 9 | 3 | 10 |\n| BabyVision | **52.3** | 34.4 | 14.2 | 49.7 |\n\n#### 일반 시각 이해 (General VQA)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| RealWorldQA | **83.9** | 83.3 | 77.0 | 83.3 |\n| MMStar | **83.8** | 77.1 | 73.2 | 83.1 |\n| HallusionBench | **71.4** | 65.2 | 64.1 | 68.6 |\n| MMBench EN | **93.7** | 88.2 | 89.2 | 93.7 |\n| SimpleVQA | **67.1** | 55.8 | 65.7 | 73.2 |\n\n#### 문서 이해·OCR\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| OmniDocBench1.5 | **90.8** | 85.7 | 87.7 | 88.5 |\n| CharXiv (RQ) | **80.8** | 82.1 | 68.5 | 81.4 |\n| MMLongBench‑Doc | **61.5** | — | 61.9 | 60.5 |\n| CC‑OCR | **82.0** | 70.3 | 76.9 | 79.0 |\n| AI2D TEST | **93.9** | 92.2 | 87.7 | 94.1 |\n| OCRBench | **93.1** | 80.7 | 85.8 | 90.4 |\n\n#### 공간 인식 (Spatial Intelligence)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| ERQA | **67.5** | 59.8 | 46.8 | 70.5 |\n| CountBench | **97.2** | 91.9 | 90.6 | 97.3 |\n| EmbSpatialBench | **84.5** | 81.3 | 75.7 | 61.2 |\n| LingoQA | **81.6** | 68.8 | 78.8 | 72.8 |\n| V* | **95.8** | 75.9 | 67.0 | 88.0 |\n\n#### 비디오 이해\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| VideoMME (w/ sub) | **87.5** | 86.0 | 77.6 | 88.4 |\n| VideoMME (w/o sub) | **83.7** | 85.8 | 81.4 | 87.7 |\n| VideoMMMU | **84.7** | 85.9 | 84.4 | 87.6 |\n| MLVU (M‑Avg) | **86.7** | 85.6 | 81.7 | 83.0 |\n| MVBench | **77.6** | 78.1 | 67.2 | 74.1 |\n| LVBench | **75.5** | 73.7 | 57.3 | 76.2 |\n\n#### 비주얼 에이전트\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| ScreenSpot Pro | **65.6** | — | 45.7 | 72.7 |\n| OSWorld‑Verified | **62.2** | 38.2 | 66.3 | — |\n| AndroidWorld | **66.8** | — | — | — |\n\n#### 의료 (Medical VQA)\n| 벤치마크 | Qwen 3.5 | GPT‑5.2 | Claude 4.5 Opus | Gemini‑3 Pro |\n|----------|----------|---------|-----------------|--------------|\n| SLAKE | **79.9** | 76.9 | 76.4 | 81.3 |\n| PMC‑VQA | **64.2** | 58.9 | 59.9 | 62.3 |\n| MedXpertQA‑MM | **70.0** | 73.3 | 63.6 | 76.0 |\n\n### 5‑3. 성능 요약\n- **비전‑수학 분야 최강**: MathVision(88.6%), MathVista(90.3%), We‑Math(87.9%)에서 GPT‑5.2와 Gemini‑3 Pro를 앞섬.\n- **문서·OCR 특화**: OmniDocBench(90.8%), OCRBench(93.1%), CC‑OCR(82.0%)에서 전 모델 대비 최고 성능.\n- **공간 인식 우수**: V*(95.8%), CountBench(97.2%), EmbSpatialBench(84.5%)에서 압도적 차이.\n- **다국어 강점**: NOVA‑63(59.1%), MAXIFE(88.2%)에서 전 모델 1위.\n- **에이전트 능력**: IFBench(76.5%), MultiChallenge(67.6%)에서 지시 수행 능력이 돋보임.\n- **추론·코딩은 GPT‑5.2에 비해 소폭 뒤처짐**: AIME26(91.3 vs 96.7), SWE‑bench Verified(76.4 vs 80.0).\n\n---\n\n## 6. 라이선스 및 데이터 사용권\n| 항목 | 내용 | 비고 |\n|------|------|------|\n| **모델 코드·가중치** | Apache 2.0 | 상업적·비상업적 모두 사용 가능 |\n| **텍스트 데이터** | CC‑BY 4.0, CC‑0, 자체 수집 | 상세 라이선스는 모델 카드 참고 |\n| **코드 데이터** | MIT, Apache 2.0, GPL 등 | 개별 레포지터리 라이선스 확인 필요 |\n\n---\n\n## 7. 제한점 및 주의사항\n- **추론 비용** – 397B 모델은 대규모 GPU 클러스터가 필요하므로, 개인 환경에서는 경량 파생 모델 사용을 권장합니다.\n- **편향·안전성** – 대규모 웹 데이터 학습 특성상 성별·인종·문화 편향이 존재할 수 있습니다.\n- **HLE 성능** – Humanity's Last Exam 벤치마크에서 28.7%로, GPT‑5.2(35.5%)·Gemini‑3 Pro(37.5%)에 비해 초고난이도 문제에서 약세를 보입니다.\n\n---\n\n## 8. 참고 자료\n- **Hugging Face Model Card** – https://huggingface.co/Qwen/Qwen3.5-397B-A17B\n- **Qwen 공식 블로그** – https://qwenlm.github.io/blog/qwen3.5/\n\n*본 문서는 2026‑02‑19 현재 Hugging Face Model Card에 공개된 정보를 기반으로 작성되었습니다.*\n",
  "lastModified": "2026-02-27T12:12:04Z",
  "author": "SEPilot AI",
  "status": "published",
  "isDraft": false,
  "isInvalid": false,
  "tags": [
    "Qwen",
    "LLM",
    "멀티모달",
    "MoE",
    "벤치마크"
  ],
  "order": 3,
  "history": [
    {
      "sha": "7bf0208",
      "message": "chore: Issue Processor 실행 결과",
      "author": "GitHub Action",
      "authorEmail": "action@github.com",
      "date": "2026-02-27T12:12:04Z",
      "isAutoCommit": false,
      "additions": 0,
      "deletions": 0
    }
  ]
}