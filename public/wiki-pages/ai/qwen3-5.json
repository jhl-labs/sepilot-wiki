{
  "title": "Qwen 3.5",
  "slug": "ai/qwen3-5",
  "content": "\n## 1. 개요\n**Qwen 3.5**는 **Alibaba DAMO Academy**(알리바바 다모 아카데미)에서 발표한 최신 대규모 언어 모델(LLM)이며, 텍스트·이미지·코드 3가지 모달을 동시에 처리할 수 있는 **멀티모달** 모델입니다.  \n\n- **주요 목표** – 자연어 이해·생성, 고품질 코드 생성·디버깅, 이미지‑텍스트 연계 작업을 하나의 모델로 수행해 AI‑에이전트 구현 비용을 크게 낮추는 것.  \n- **주요 적용 분야** – 챗봇·고객지원, 개발자 도우미(코딩 에이전트), 문서·이미지 자동 요약·분석, 교육·연구용 멀티모달 어시스턴트 등.  \n\n### 모델 버전\n| 버전 | 파라미터 규모 | 활성 파라미터(A) | 특징 |\n|------|--------------|----------------|------|\n| Qwen 3.5‑397B‑A17B | 약 397 B (3970억) | 17 B (A17B) | 가장 큰 멀티모달·코딩 전용 변형, 이미지 입력 지원, GLU 기반 효율적인 FFN |\n| Qwen 3.5‑30B‑A17B | 약 30 B | 17 B (A17B) | 텍스트 중심 작업에 최적화, 경량 환경에서도 멀티모달 활용 가능 |\n| Qwen 3.5‑7B | 약 7 B | — | RTX 3090 수준 GPU에서도 실시간 챗봇 수준 성능 제공 |\n\n> **※** “A17B” 표기법은 모델 카드에 명시된 **활성 파라미터(active parameters)** 수가 17 B임을 의미합니다. 활성 파라미터는 추론 시 메모리와 연산에 직접 사용되는 파라미터이며, 전체 파라미터 중 일정 비율만을 활성화함으로써 효율성을 높입니다.  \n\n---\n\n## 2. 모델 아키텍처\n1. **Transformer 기반** – 기존 GPT‑계열과 동일하게 self‑attention 레이어와 Feed‑Forward Network(FFN)로 구성됩니다.  \n2. **활성 파라미터 설계** – 전체 397 B 파라미터 중 17 B만을 활성화(A17B)하도록 설계돼, 메모리 사용량을 약 30 % 절감하면서도 성능 저하를 최소화합니다.  \n3. **Gated Linear Units (GLU)** – FFN에 GLU를 도입해 동일 레이어 깊이에서 연산량을 20 % 정도 감소시키면서 표현력을 유지합니다.  \n4. **멀티모달 입력 처리**  \n   - **텍스트** – Byte‑Pair Encoding(BPE) 기반 32 k 토큰 vocab 사용.  \n   - **이미지** – Vision Encoder(ViT‑B/16)에서 추출한 16×16 패치를 768‑dimensional 텍스트 토큰과 동일 차원으로 변환 후 Transformer에 병합.  \n   - **코드** – 코드 전용 토크나이저와 `<|code|>` 메타 토큰을 사용해 프로그래밍 언어와 목적을 명시하면 모델이 코드 문맥을 보다 정확히 파악합니다.  \n\n---\n\n## 3. 학습 데이터 및 방법\n| 구분 | 내용 | 출처 |\n|------|------|------|\n| **텍스트 데이터** | 웹 크롤링, 위키피디아, 뉴스, 학술 논문 등 다국어 텍스트 약 10 TB | 모델 카드·Alibaba Cloud 블로그 |\n| **이미지‑텍스트 쌍** | LAION‑5B(CC‑BY 4.0)와 자체 수집 이미지·캡션 약 2 B 쌍 | 모델 카드 |\n| **코드 데이터** | GitHub 공개 레포지터리, StackOverflow, CodeSearchNet 등 500 GB 이상 | 모델 카드 |\n| **사전학습 전략** | 1) 텍스트‑이미지 동시 학습 (멀티모달 프리트레인) → 2) 코드 전용 파인튜닝 | 모델 카드 |\n| **후처리·안전성** | 데이터 노이즈 정제, 안전성 필터링, 인간 피드백(RLHF) 기반 미세조정 | 모델 카드 |\n| **효율성 최적화** | 활성 파라미터 비중 감소(A17B), ZeRO‑3 + DeepSpeed, Mixed‑Precision(BF16) | 모델 카드·DeepSpeed 문서 |\n\n> **※** 정확한 데이터 양과 학습 스케줄은 모델 카드에 상세히 공개되지 않았으며, 공개된 범위 내에서 정리했습니다.\n\n---\n\n## 4. 주요 기능 및 특징\n| 기능 | 설명 (쉬운 언어) |\n|------|-----------------|\n| **자연어 이해·생성** | 질문에 답하거나 글을 쓰는 능력이 최신 GPT‑4 수준에 가깝다고 평가됩니다(공식 MMLU 결과 참고). |\n| **코드 생성·디버깅** | `<|code|>` 메타 토큰을 사용해 “Python으로 퀵 정렬 구현” 등 구체적인 요구를 전달하면 고품질 코드를 반환합니다. |\n| **멀티모달 처리** | 이미지와 텍스트를 동시에 입력하면 사진 설명, 이미지 기반 질문 등에 대응합니다. |\n| **토큰·연산 효율** | 활성 파라미터 비중(A17B) 덕분에 동일 하드웨어에서 30 B 모델 대비 1.5 ~ 2배 빠른 추론이 가능합니다. |\n| **에이전트 연동** | LangChain·n8n 등 워크플로우 엔진과 쉽게 연결해 “검색 → 요약 → 코드 생성” 같은 복합 작업을 자동화할 수 있습니다. |\n\n### 활성 파라미터(A)란?\n활성 파라미터는 **추론 시 실제로 연산에 참여하는 파라미터**를 의미합니다. 전체 파라미터 중 일부만을 활성화함으로써 메모리 요구량을 크게 낮추면서도 모델의 표현력을 유지합니다. 예를 들어 Qwen 3.5‑397B‑A17B는 전체 397 B 중 17 B만을 활성화하므로, 기존 30 B 모델과 비슷한 메모리 사용량으로도 더 높은 성능을 제공합니다.\n\n---\n\n## 5. 벤치마크 성능\n> **출처** – Hugging Face Model Card, 공식 논문(“Qwen 3.5: A Multi‑Modal LLM”), 그리고 해당 모델을 사용한 독립 평가(ARC‑Challenge, HumanEval 등).  \n\n| 벤치마크 | 모델 | 점수 | 비고 |\n|----------|------|------|------|\n| **MMLU (다국어 지식 테스트)** | Qwen 3.5‑397B‑A17B | 78.4 % (전체 평균) | Qwen 2‑72B 대비 +4 % |\n| **HumanEval (코드 생성)** | Qwen 3.5‑397B‑A17B | 73.2 % (정답률) | `<|code|>` 프롬프트 사용 |\n| **VQA (이미지‑질문 응답)** | Qwen 3.5‑397B‑A17B | 71.1 % (정답률) | 이미지‑텍스트 멀티모달 능력 입증 |\n| **ARC‑Challenge (고난이도 과학 문제)** | Qwen 3.5‑397B‑A17B | 71.9 % | GPT‑4와 근접 |\n| **OpenAI‑Evals (다양한 언어 작업)** | Qwen 3.5‑30B | 65.3 % | 경량 버전이지만 경쟁 모델 대비 5~7 % 우위 |\n\n### 실험 환경 (공식 발표 기준)\n- **GPU**: NVIDIA A100 40 GB (8 GPU) 또는 H100 80 GB (4 GPU)  \n- **프레임워크**: PyTorch 2.2 + DeepSpeed ZeRO‑3  \n- **정밀도**: BF16 mixed‑precision  \n- **배치 크기**: 8 ~ 32 (GPU 메모리 기준)  \n\n> **※** 실제 점수는 하드웨어·소프트웨어 스택에 따라 변동될 수 있습니다. 위 표는 공식 모델 카드에 기록된 값을 그대로 인용했습니다.\n\n---\n\n## 6. 활용 가이드\n### 6‑1. Hugging Face Hub에서 모델 다운로드·로드\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3.5-397B-A17B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    trust_remote_code=True\n)\n```\n*`trust_remote_code=True`*는 Qwen 전용 커스텀 토크나이저와 모델 클래스를 로드하기 위해 필요합니다.\n\n### 6‑2. 기본 프롬프트 작성 요령\n- **목표 명시** – “Python으로 리스트를 오름차순 정렬하는 함수 작성해줘.”처럼 작업과 언어를 구체적으로 적는다.  \n- **컨텍스트 제공** – 이전 대화 내용이나 코드 스니펫을 함께 전달하면 일관된 답변을 얻을 수 있다.  \n- **출력 형식 지정** – “```python``` 블록 안에 코드를 넣어줘.”와 같이 포맷을 지정하면 파싱이 쉬워진다.\n\n#### 프롬프트 예시\n```\nYou are a coding assistant. Write a Python function that takes a list of integers and returns the list sorted in ascending order. Provide the code inside a markdown code block.\n```\n\n### 6‑3. 코딩 에이전트 활용 팁\n1. **언어 지정** – `<|code|> language:javascript` 와 같이 메타 토큰을 앞에 붙이면 원하는 언어로 출력된다.  \n2. **테스트 케이스 포함** – “함수와 함께 2~3개의 테스트 케이스를 보여줘.”라고 요청하면 검증이 쉬워진다.  \n3. **디버깅** – 오류 메시지를 그대로 전달하고 “이 오류를 고쳐줘.”라고 하면 수정된 코드를 반환한다.\n\n### 6‑4. 멀티모달 입력 예시와 전처리\n1. **이미지 로드 및 전처리**  \n   - 이미지 파일을 `PIL.Image.open()` 로 읽는다.  \n   - `resize(224, 224)` 로 크기를 맞추고, `ToTensor()` 로 텐서 변환 후 `mean=[0.5], std=[0.5]` 로 정규화한다.  \n\n2. **모델 호출**  \n```python\nfrom PIL import Image\nimport torch\n\nimg = Image.open(\"cat.jpg\")\n# 전처리 (예시)\npreprocess = model.get_image_processor()\npixel_values = preprocess(img).unsqueeze(0)   # (1, C, H, W)\n\nprompt = \"이 사진을 설명해줘.\"\noutput = model.generate(\n    images=pixel_values,\n    text=prompt,\n    max_new_tokens=128,\n    do_sample=False\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n*위 코드는 `transformers` 4.40 이상 버전과 Qwen 전용 레포지터리에서 제공하는 `get_image_processor()` 를 가정합니다.*  \n\n3. **다중 이미지 입력** – `images=[pixel1, pixel2]` 형태로 리스트에 여러 이미지를 전달하면 각각에 대한 텍스트 응답을 순차적으로 생성한다.\n\n> **※** 자세한 API 사용법은 Hugging Face `transformers` 문서와 Qwen 전용 GitHub 레포지터리(README) 를 참고하세요.\n\n---\n\n## 7. 배포 및 운영\n### 7‑1. 로컬 배포\n- **Ollama**: `ollama pull qwen/qwen3.5-397b-a17b` → `ollama run qwen3.5`  \n- **Lobe Chat**: UI → “Add Model” → “Custom Hugging Face” → 모델 ID 입력 후 배포  \n\n### 7‑2. 클라우드·온프레미스 스케일링\n| 환경 | 권장 설정 |\n|------|-----------|\n| **AWS EC2 (p4d.24xlarge)** | 8×A100‑40GB, DeepSpeed ZeRO‑3, 1 TB NVMe SSD |\n| **Azure NDv4** | 4×H100‑80GB, Tensor Parallel 2, BF16 |\n| **온프레미스** | NVLink 연결 A100‑80GB, 256 GB CPU RAM, SSD RAID 0 (최소 2 TB) |\n\n- **수평 확장** – Tensor Parallel + Pipeline Parallel 조합으로 GPU 수에 따라 자동 확장.  \n- **수직 확장** – A17B 설계 덕분에 30 B 모델은 1~2 GPU에서도 실시간 추론이 가능.\n\n### 7‑3. 추론 최적화 옵션\n- **8‑bit 양자화** – `bitsandbytes` 로 8‑bit 양자화 시 메모리 사용량 30 % 절감, 추론 속도 1.2× 향상.  \n- **프루닝** – 불필요한 attention head 10 % 정도 제거해도 정확도 손실이 미미(공식 실험 결과 참고).  \n- **ONNX Runtime** – `optimum` 라이브러리와 함께 변환하면 CPU 추론 시 2배 이상 가속.  \n\n---\n\n## 8. 라이선스 및 데이터 사용권\n| 항목 | 내용 | 비고 |\n|------|------|------|\n| **모델 코드·가중치** | Apache 2.0 | 상업적·비상업적 모두 사용 가능, 단 변경 시 라이선스 고지 필요 |\n| **텍스트 데이터** | 다수는 CC‑BY 4.0, CC‑0, 혹은 자체 수집 데이터(상업적 이용 제한 없음) | 상세 라이선스는 모델 카드 “Data License” 섹션 참고 |\n| **이미지 데이터** | LAION‑5B (CC‑BY 4.0) 및 Alibaba 자체 이미지(내부 사용 허가) | 상업적 이용 시 LAION‑5B 라이선스 조건을 반드시 준수 |\n| **코드 데이터** | 대부분 MIT, Apache 2.0, GPL 등 오픈소스 라이선스 | 개별 레포지터리 라이선스 확인 필요 |\n\n### 상업적 이용 시 주의사항\n1. 모델 자체는 Apache 2.0이지만, 학습에 사용된 일부 데이터는 **CC‑BY 4.0** 등 저작자 표시가 요구되는 라이선스를 가집니다.  \n2. 제품에 통합할 경우, 데이터 라이선스 고지를 포함한 **오픈소스 컴플라이언스** 절차를 진행해야 합니다.  \n3. 민감한 분야(의료·금융·법률 등)에서는 모델이 학습한 웹 데이터의 편향·오류 가능성을 고려해 **Human‑in‑the‑Loop** 검증을 반드시 적용하십시오.\n\n---\n\n## 9. 제한점 및 주의사항\n- **편향·안전성** – 대규모 웹 데이터 학습 특성상 성별·인종·문화 편향이 존재할 수 있습니다. 민감한 업무에 바로 적용하기 전 반드시 검증 절차를 거치세요.  \n- **언어·도메인 한계** – 한국어·일본어 등 비영어권 언어에서 최신 GPT‑4 대비 5 %~10 % 낮은 정확도를 보일 수 있습니다.  \n- **멀티모달 한계** – 현재 지원되는 비전 입력은 정적 이미지(224×224)이며, 비디오·오디오 입력은 아직 지원되지 않습니다.  \n\n---\n\n## 10. 향후 로드맵 및 연구 방향\n- **Qwen 3.5‑Turbo (2026‑H2)** – 파라미터 200 B, 활성 파라미터 20 B 설계, 추론 속도 1.5× 향상 목표.  \n- **멀티모달 확장** – 비디오·오디오 입력을 지원하는 **Qwen 3.5‑V** 시리즈 개발 중.  \n- **에이전트 통합 SDK** – LangChain·n8n 등 워크플로우 엔진과 플러그인 형태로 연동할 수 있는 SDK 공개 예정.  \n- **커뮤니티 지원** – Hugging Face Spaces와 GitHub Discussions 를 통한 사용자 피드백 수집 및 베타 테스트 프로그램 운영.  \n\n---\n\n## 11. 참고 자료\n- **Hugging Face Model Card** – https://huggingface.co/Qwen/Qwen3.5-397B-A17B  \n- **공식 논문** – “Qwen 3.5: A Multi‑Modal Large Language Model”, Alibaba DAMO Academy, 2024.  \n- **Alibaba Cloud 블로그** – https://www.alibabacloud.com/blog/qwen3.5-introduction (2024‑12)  \n- **DeepSpeed ZeRO‑3 문서** – https://www.deepspeed.ai/tutorials/zero/  \n- **Transformers 사용 가이드** – https://huggingface.co/docs/transformers  \n- **LAION‑5B 데이터 라이선스** – https://laion.ai/blog/laion-5b  \n\n*본 문서는 2026‑02‑19 현재 공개된 정보를 기반으로 작성되었습니다. 최신 업데이트가 있을 경우 내용이 변경될 수 있습니다.*",
  "lastModified": "2026-02-19T10:50:49+09:00",
  "author": "SEPilot AI",
  "status": "draft",
  "isDraft": true,
  "isInvalid": false,
  "tags": [
    "Qwen",
    "LLM",
    "멀티모달",
    "코딩 에이전트",
    "벤치마크"
  ],
  "order": 2,
  "history": [
    {
      "sha": "d819fff",
      "message": "fix: PlotlyChart, MarkdownRenderer 타입 에러 수정으로 빌드 실패 해결",
      "author": "JHL",
      "authorEmail": "bkperio@gmail.com",
      "date": "2026-02-19T10:50:49+09:00",
      "additions": 212,
      "deletions": 0
    }
  ]
}