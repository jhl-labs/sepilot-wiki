{
  "title": "Google TPU에서 Tunix를 이용한 FunctionGemma 파인튜닝 가이드",
  "slug": "ai/fine-tuning-functiongemma-on-google-tpu-with-tunix",
  "content": "\n## 1. 개요\nFunctionGemma는 **작고 효율적인 언어 모델**로, 자연어를 바로 실행 가능한 API 호출 형태로 변환합니다. 기존에는 Hugging Face TRL을 활용해 **GPU** 환경에서 파인튜닝하는 방법이 주로 소개되었습니다[[euno.news](https://euno.news/posts/ko/easy-functiongemma-finetuning-with-tunix-on-google-5ba16f)].\n\n이번 가이드에서는 **Google TPU**와 **JAX 기반 경량 라이브러리 Tunix**를 사용해 **LoRA**(Low‑Rank Adaptation) 방식으로 **FunctionGemma‑270M‑IT** 모델을 파인튜닝하는 전체 워크플로우를 다룹니다. 무료 티어인 **Colab TPU v5e‑1**에서도 전체 과정을 수행할 수 있어 비용 효율성이 크게 향상됩니다[[euno.news](https://euno.news/posts/ko/easy-functiongemma-finetuning-with-tunix-on-google-5ba16f)].\n\n## 2. 사전 준비\n| 항목 | 내용 | 비고 |\n|------|------|------|\n| Google Cloud 계정 | Colab 사용 시 자동 연결되지만, 필요 시 **Cloud TPU** 인스턴스를 직접 생성할 수 있음 | 무료 티어는 Colab TPU v5e‑1 기준 |\n| Hugging Face 계정 | 모델·데이터셋 다운로드 및 `hf_hub_download`·`snapshot_download` 사용 | 공개 모델·데이터셋은 인증 없이 접근 가능 |\n| Python 환경 | Python 3.10 이상 권장 | JAX·Tunix는 최신 Python과 호환 |\n| 지원 TPU 종류 | **v5e‑1** (Colab 무료 티어) 외 v4‑8, v4‑32 등 | 모델·배치 크기에 따라 선택 |\n\n## 3. 패키지 호환 매트릭스\n> **주의**: 아래 버전은 2026‑02‑03 기준 최신 안정화 버전이며, 실제 환경에서는 `pip install -U` 로 최신 패키지를 확인하세요.\n\n| 패키지 | 권장 버전 |\n|--------|------------|\n| `jax[tpu]` | `0.4.*` |\n| `jaxlib` | `0.4.*` (TPU 지원) |\n| `tunix` | 최신 (>=0.1) |\n| `huggingface_hub` | 최신 |\n| `safetensors` | 최신 |\n| `optax` | 최신 |\n| `datasets` / `evaluate` | 최신 |\n\n## 4. 환경 설정 (Colab)\n1. **런타임** → **런타임 유형 변경** → **하드웨어 가속기** → **TPU** 선택\n2. **필수 패키지 설치**\n```python\n!pip install -q \"jax[tpu]==0.4.*\" \"jaxlib==0.4.*\" tunix huggingface_hub safetensors optax datasets evaluate\"\n```\n3. **TPU 초기화 및 설정**\n```python\nimport jax\n# TPU 디바이스 확인\nprint(\"TPU devices:\", jax.devices())\n# XLA 플래그 (필요 시) – 예시\nimport os\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n# JAX 플랫폼 명시 (명시적 설정)\njax.config.update('jax_platform_name', 'tpu')\n```\n4. **에러 대응 팁**\n   - `ImportError` 발생 시 `pip install -U jax[tpu] jaxlib` 로 버전 일치\n   - 메모리 부족(OOM) 시 `batch_size` 감소 또는 `max_length` 축소\n   - `jax.devices()` 가 빈 리스트이면 런타임이 TPU가 아닌 CPU로 실행 중임을 의미하므로 런타임 설정을 재검토\n\n## 5. 데이터셋 준비\n```python\nfrom huggingface_hub import snapshot_download, hf_hub_download\nimport json\n\nMODEL_ID = \"google/functiongemma-270m-it\"\nDATASET_ID = \"google/mobile-actions\"\n\n# 모델 가중치 다운로드\nlocal_model_path = snapshot_download(repo_id=MODEL_ID, ignore_patterns=[\"*.pth\"])\n# 데이터셋 다운로드 (JSONL 형식)\ndata_file = hf_hub_download(repo_id=DATASET_ID, filename=\"dataset.jsonl\", repo_type=\"dataset\")\n```\n### 5.1 전처리 및 토크나이저\n```python\nfrom transformers import AutoTokenizer\nimport numpy as np\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n\nclass MobileActionDataset:\n    def __init__(self, file_path, tokenizer, max_length=1024):\n        self.samples = []\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                obj = json.loads(line)\n                # prompt: 사용자 질의, completion: API 호출 문자열\n                self.samples.append((obj[\"prompt\"], obj[\"completion\"]))\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        prompt, completion = self.samples[idx]\n        # 전체 텍스트를 하나의 시퀀스로 토크나이징 (프롬프트+완료)\n        tokenized = self.tokenizer(\n            prompt + completion,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"np\",\n        )\n        return tokenized\n```\n\n## 6. 모델 로드 및 LoRA 적용\n```python\nimport tunix.params_safetensors_lib as safetensors_lib\nfrom tunix import nnx, qwix\nimport jax\n\n# 모델 구성 로드 (config.json 포함)\nmodel_config = safetensors_lib.load_config(local_model_path)\n\n# TPU 메쉬 정의 (sharding 지원)\nNUM_TPUS = len(jax.devices())\nMESH = [(1, NUM_TPUS), (\"fsdp\", \"tp\")] if NUM_TPUS > 1 else [(1, 1), (\"fsdp\", \"tp\")]\nmesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0]))\n\nwith mesh:\n    base_model = safetensors_lib.create_model_from_safe_tensors(\n        local_model_path,\n        model_config,\n        mesh,\n    )\n\n# LoRA 하이퍼파라미터 (기본값) – 필요 시 조정\nLORA_RANK = 8\nLORA_ALPHA = 16\n\nlora_provider = qwix.LoraProvider(\n    module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\",\n    rank=LORA_RANK,\n    alpha=LORA_ALPHA,\n)\n\nmodel_input = base_model.get_model_input()\nmodel = qwix.apply_lora_to_model(\n    base_model,\n    lora_provider,\n    rngs=nnx.Rngs(0),\n    **model_input,\n)\n\n# 상태와 파티셔닝 정보 확보\nstate = nnx.state(model)\npspecs = nnx.get_partition_spec(state)\nsharded_state = jax.lax.with_sharding_constraint(state, pspecs)\nnnx.update(model, sharded_state)\n```\n\n## 7. 학습 파이프라인\n### 7.1 하이퍼파라미터 기본값\n| 파라미터 | 기본값 |\n|----------|--------|\n| `batch_size` | 8 |\n| `learning_rate` | 5e-4 |\n| `num_epochs` | 3 |\n| `warmup_steps` | 100 |\n| `lora_rank` | 8 |\n| `lora_alpha` | 16 |\n| `max_length` | 1024 |\n\n### 7.2 옵티마이저 & 스케줄\n```python\nimport optax\n\n# Linear warmup + cosine decay\nschedule = optax.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=5e-4,\n    warmup_steps=100,\n    decay_steps=1000,  # 전체 스텝 수에 맞게 조정\n)\noptimizer = optax.adamw(schedule, weight_decay=0.01)\nopt_state = optimizer.init(state.params)\n```\n### 7.3 손실 함수 (completion‑only)\n```python\nimport jax.numpy as jnp\n\ndef completion_only_loss(logits, labels, mask):\n    log_probs = jax.nn.log_softmax(logits, axis=-1)\n    token_loss = -jnp.sum(log_probs * jax.nn.one_hot(labels, logits.shape[-1]), axis=-1)\n    return jnp.mean(token_loss * mask)\n```\n### 7.4 배치 생성기\n```python\nimport numpy as np\n\ndef data_generator(dataset, batch_size, tokenizer):\n    while True:\n        idxs = np.random.choice(len(dataset), batch_size, replace=False)\n        batch = [dataset[i] for i in idxs]\n        # 각 샘플은 이미 tokenized dict 형태\n        input_ids = np.stack([b[\"input_ids\"] for b in batch])\n        attention_mask = np.stack([b[\"attention_mask\"] for b in batch])\n        # 라벨은 입력과 동일 (completion‑only 마스크는 아래에서 생성)\n        yield {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n        }\n```\n### 7.5 학습 스텝\n```python\nimport os\n\ndef train_step(state, opt_state, batch):\n    def loss_fn(params):\n        logits = model.apply(params, batch[\"input_ids\"], rngs=nnx.Rngs(0))\n        # 마스크: 프롬프트 부분을 0, completion 부분을 1 로 가정 (예시)\n        mask = batch[\"attention_mask\"]  # 실제로는 프롬프트 마스크를 조정 필요\n        loss = completion_only_loss(logits, batch[\"input_ids\"], mask)\n        return loss, logits\n    (loss, _), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n    updates, new_opt_state = optimizer.update(grads, opt_state, state.params)\n    new_params = optax.apply_updates(state.params, updates)\n    new_state = state.replace(params=new_params)\n    return new_state, new_opt_state, loss\n```\n### 7.6 전체 학습 루프 (예시)\n```python\ntrain_dataset = MobileActionDataset(data_file, tokenizer)\ntrain_gen = data_generator(train_dataset, batch_size=8, tokenizer=tokenizer)\n\nnum_steps = 1000  # 예시, 실제는 `num_epochs * len(train_dataset) // batch_size`\nfor step in range(1, num_steps + 1):\n    batch = next(train_gen)\n    state, opt_state, loss = train_step(state, opt_state, batch)\n    if step % 100 == 0:\n        print(f\"Step {step}/{num_steps} – loss: {loss:.4f}\")\n    if step % 500 == 0:\n        ckpt_dir = f\"/content/checkpoints/step_{step}\"\n        os.makedirs(ckpt_dir, exist_ok=True)\n        nnx.save(state, ckpt_dir)\n        print(f\"Checkpoint saved at {ckpt_dir}\")\n```\n\n## 8. 평가 및 검증\n### 8.1 평가 메트릭\n```python\nfrom datasets import load_metric\nbleu = load_metric(\"bleu\")\nexact_match = load_metric(\"accuracy\")\n```\n### 8.2 검증 루프 (간단 예시)\n```python\ndef evaluate(state, dataset, tokenizer, batch_size=8):\n    gen = data_generator(dataset, batch_size, tokenizer)\n    total_bleu, total_em, count = 0.0, 0.0, 0\n    for _ in range(100):  # 샘플 수 제한 (예시)\n        batch = next(gen)\n        logits = model.apply(state.params, batch[\"input_ids\"], rngs=nnx.Rngs(0))\n        pred_ids = jnp.argmax(logits, axis=-1)\n        preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n        refs = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n        total_bleu += bleu.compute(predictions=preds, references=[[r] for r in refs])[\"bleu\"]\n        total_em += exact_match.compute(predictions=preds, references=refs)[\"accuracy\"]\n        count += 1\n    print(f\"BLEU: {total_bleu/count:.4f}, Exact Match: {total_em/count:.4f}\")\n```\n### 8.3 샘플 인퍼런스\n```python\ndef generate_api_call(prompt: str):\n    tokenized = tokenizer(prompt, return_tensors=\"np\")\n    logits = model.apply(state.params, tokenized[\"input_ids\"], rngs=nnx.Rngs(0))\n    generated_ids = jnp.argmax(logits, axis=-1)\n    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\nsample_prompt = \"Turn on Bluetooth and open the camera.\"\nprint(\"Generated API call:\", generate_api_call(sample_prompt))\n```\n\n## 9. TPU 성능 지표 (참고)\n- **Throughput**: 약 1‑2 samples/second (무료 티어 기준, 실제는 배치·시퀀스 길이에 따라 변동) \n- **Latency**: 200‑400 ms 수준 (실험에 따라 차이) \n> 정확한 수치는 실행 환경에 따라 달라지므로, `jax.profiler` 로 프로파일링을 권장합니다.\n\n## 10. 모델 배포 옵션\n| 옵션 | 설명 | 장점 | 단점 |\n|------|------|------|------|\n| Colab TPU 직접 서비스 | 학습 후 동일 노트북에서 `jax.jit` 기반 inference | 설정 간단, 비용 없음 | 세션 종료 시 사라짐 |\n| Cloud TPU 인스턴스 | 별도 프로젝트에 TPU 클러스터 생성 후 장기 운영 | 자동 스케일링, 지속성 | 사용량 기반 비용 발생 |\n| SavedModel / JAX‑to‑TF 변환 | `jax2tf` 로 TensorFlow SavedModel 생성 | Edge 디바이스(Android, iOS) 배포 가능 | 변환 시 일부 연산 호환성 이슈 가능 |\n| Edge 디바이스 직접 배포 | 변환된 모델을 모바일·IoT에 탑재 | 로컬 추론, 네트워크 비용 절감 | 메모리·연산 제한에 맞춘 경량화 필요 |\n\n## 11. 트러블슈팅 & 베스트 프랙티스\n| 문제 | 원인 | 해결 방법 |\n|------|------|-----------|\n| **sharding mismatch** | 메쉬 정의와 파라미터 파티셔닝 불일치 | `mesh` 정의 재검토, `nnx.get_partition_spec` 출력 확인 |\n| **OOM** | 배치·시퀀스 길이가 TPU 메모리 초과 | 배치 크기 감소, `max_length` 축소, `gradient_checkpointing` 활용 |\n| **JAX/XLA 버전 충돌** | `jax`와 `jaxlib` 버전 불일치 | `pip install -U \"jax[tpu]==0.4.*\" \"jaxlib==0.4.*\"` 로 동일 버전 재설치 |\n| **LoRA 적용 실패** | `module_path` 정규식이 모델 구조와 맞지 않음 | `print(base_model)` 로 레이어 이름 확인 후 정규식 수정 |\n| **학습 속도 저하** | `pmap` 대신 `pjit` 미사용, 디바이스 간 통신 병목 | `jax.profiler` 로 병목 파악 후 `pjit` 전환 검토 |\n\n### 베스트 프랙티스 요약\n- **단일 TPU(v5e‑1)에서 작은 배치·짧은 epoch** 으로 빠르게 검증\n- **LoRA rank/alpha** 기본값 8/16 사용, 필요 시 메모리·성능에 맞게 조정\n- **로그 모니터링**: `jax.debug.print` 와 TensorBoard (`%tensorboard --logdir logs`) 활용\n- **체크포인트**: 500 스텝마다 저장, `nnx.save` 로 전체 파라미터와 옵티마이저 상태 보존\n\n## 12. 라이선스 및 참고 문헌\n- **FunctionGemma 모델**: Hugging Face Hub 에서 제공되는 모델 라이선스는 해당 페이지에 명시된 **Apache‑2.0** 또는 **MIT** 등 공개 라이선스를 따릅니다. 사용 전 반드시 모델 페이지의 `LICENSE` 파일을 확인하세요.\n- **Mobile‑Action 데이터셋**: 데이터셋 페이지에 명시된 **Creative Commons Attribution 4.0 (CC‑BY‑4.0)** 라이선스를 따릅니다.\n- **Tunix**: Google Open Source 라이선스 (Apache‑2.0) 적용 – 자세한 내용은 GitHub 레포지터리 `LICENSE` 파일 참고.\n\n### 참고 문헌\n1. Euno News, “Google TPU에서 Tunix를 활용한 Easy FunctionGemma 파인튜닝”, 2026‑02‑03, <https://euno.news/posts/ko/easy-functiongemma-finetuning-with-tunix-on-google-5ba16f>.\n2. Google Developers Blog, “Easy FunctionGemma fine‑tuning with Tunix on Google TPUs”, 2026, <https://developers.googleblog.com/easy-functiongemma-finetuning-with-tunix-on-google-tpus/>.\n3. LoRA 논문, *Low‑Rank Adaptation of Large Language Models*, 2021.\n4. JAX 공식 문서, <https://jax.readthedocs.io/>.\n5. Optax 최적화 라이브러리, <https://optax.readthedocs.io/>.\n6. Hugging Face Hub, FunctionGemma‑270M‑IT 모델 페이지, <https://huggingface.co/google/functiongemma-270m-it>.\n7. Hugging Face Hub, Mobile‑Action 데이터셋 페이지, <https://huggingface.co/datasets/google/mobile-actions>.\n\n---\n*본 문서는 2026‑02‑03 기준 공개된 자료를 기반으로 작성되었습니다. 최신 버전·옵션에 대한 상세 내용은 각 공식 문서를 참고하시기 바랍니다.*",
  "lastModified": "2026-02-25T02:59:38Z",
  "author": "GitHub Action",
  "status": "draft",
  "isDraft": true,
  "isInvalid": false,
  "tags": [
    "FunctionGemma",
    "TPU",
    "Tunix",
    "JAX",
    "LoRA",
    "파인튜닝"
  ],
  "history": [
    {
      "sha": "26adf37",
      "message": "docs: Issue #261 - 피드백 반영",
      "author": "GitHub Action",
      "authorEmail": "action@github.com",
      "date": "2026-02-25T02:59:38Z",
      "isAutoCommit": false,
      "additions": 0,
      "deletions": 0
    },
    {
      "sha": "0b7c05b",
      "message": "chore: Issue Processor 실행 결과",
      "author": "GitHub Action",
      "authorEmail": "action@github.com",
      "date": "2026-02-24T06:52:34Z",
      "isAutoCommit": false,
      "additions": 0,
      "deletions": 0
    }
  ]
}