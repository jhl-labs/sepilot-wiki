{
  "title": "Open WebUI와 Docker Model Runner를 활용한 셀프‑호스티드 LLM 가이드",
  "slug": "docker/308",
  "content": "\n## 1. 개요\n이 문서는 **Open WebUI**와 **Docker Model Runner**를 결합해 로컬에서 대형 언어 모델(LLM)을 안전하게 운영하고자 하는 개발자·운영자를 대상으로 합니다.  \n- **목적**: Docker 기반 마이크로VM 샌드박스를 이용해 Claude Code, Gemini, Codex, Kiro 등 최신 코딩 에이전트를 감독 없이(unsupervised) 실행하면서도 격리와 보안을 유지하는 방법을 제공한다.  \n- **대상 독자**: Docker 사용 경험이 있는 엔지니어, AI·LLM 실험을 로컬 환경에서 진행하려는 연구자, 보안·규정 준수가 필요한 기업 사용자.  \n\n셀프‑호스티드 LLM 운영은  \n- 클라우드 비용 절감,  \n- 데이터 유출 위험 최소화,  \n- 커스텀 프롬프트·플러그인 적용 자유  \n와 같은 기대 효과를 제공합니다 [출처: euno.news](https://euno.news/posts/ko/open-webui-docker-model-runner-self-hosted-models-a92a5a).\n\n## 2. 사전 요구사항\n| 항목 | 권장/필수 | 비고 |\n|------|-----------|------|\n| 운영체제 | Linux (Ubuntu 20.04 이상 권장) | Docker Engine은 Linux 커널 기능을 활용합니다. |\n| Docker Engine | **Docker Engine 24.x 이상** (Docker Desktop 포함) | 최신 Hardened Images와 microVM 기능은 최신 엔진에서 지원됩니다 [Docker Blog, 2025‑12‑17]. |\n| CPU | x86_64 또는 ARM64, **멀티코어 4코어 이상** 권장 | 모델 로드·추론 시 멀티스레드 활용. |\n| GPU | NVIDIA CUDA 11.x 이상 (옵션) | GPU 가속이 필요할 경우 `nvidia-docker2` 설치 필요. |\n| 메모리 | 최소 **8 GB**, 권장 **16 GB 이상** | 모델 크기에 따라 차등. |\n| 저장소 | SSD ≥ 50 GB, 모델 캐시용 별도 볼륨 권장 | 모델 파일은 수십 GB에 달할 수 있음. |\n| 네트워크 | 포트 **8080**(Open WebUI) 및 **5000**(Model Runner API) 개방 | 방화벽 규칙에 예외 추가 필요. |\n| 계정 | Docker Hub 계정 (이미지 pull용) | 무료 Hardened Images는 Docker Hub에서 제공됩니다 [Docker Blog, 2025‑12‑17]. |\n| 기타 도구 | `git` (레포지터리 클론), `curl` (헬스체크) |  |\n\n> **※ 추가 조사 필요**: 정확한 Docker Engine 최소 버전, GPU 드라이버 호환 매트릭스, 권장 OS 배포판 등은 공식 Docker 문서와 모델 러너 릴리즈 노트를 확인해야 합니다.\n\n## 3. Docker Model Runner 소개\n### 3.1 전체 아키텍처\n```\n+-------------------+      +-------------------+      +-------------------+\n|   Open WebUI      | <--->| Docker Model Runner| <--->|  MicroVM Sandbox  |\n+-------------------+      +-------------------+      +-------------------+\n        ^                         ^                         ^\n        |                         |                         |\n   사용자 UI                API (REST)               격리된 모델 프로세스\n```\n- **Open WebUI**: 웹 기반 프론트엔드, 사용자 인증·프롬프트 관리.  \n- **Docker Model Runner**: 모델 파일을 로드하고 HTTP / gRPC API를 제공하는 경량 컨테이너.  \n- **MicroVM Sandbox**: Docker가 제공하는 **microVM 기반 격리**(Firecracker 등) 위에 모델 프로세스를 실행, “Unsupervised하지만 Safe” 환경을 구현 [Docker Blog, 2026‑01‑30].\n\n### 3.2 주요 컴포넌트\n| 컴포넌트 | 역할 |\n|----------|------|\n| `model-runner` | 모델 로드, 추론 엔드포인트 제공 |\n| `microVM` | 하드웨어 가상화 없이 경량 VM 형태로 격리 |\n| `sandbox‑controller` | 컨테이너·microVM lifecycle 관리, 정책 적용 |\n\n### 3.3 이미지 구조와 레이어\nDocker Hardened Images는 **베이스 이미지 + 보안 패치 레이어** 형태로 제공됩니다. 이미지 레이어는 다음과 같이 구성됩니다.\n1. `docker.io/library/ubuntu:22.04` (베이스)  \n2. `docker.io/hardened/model-runner:latest` (Hardened Image) – 최신 보안 설정 포함 [Docker Blog, 2025‑12‑17].\n\n### 3.4 실행 흐름\n1. **컨테이너 생성** – `docker run` 명령으로 Hardened Image 실행.  \n2. **MicroVM 초기화** – `sandbox‑controller`가 microVM을 스폰하고 네임스페이스를 할당.  \n3. **모델 로드** – 지정된 모델 레지스트리(HF, Ollama 등)에서 파일을 다운로드하고 캐시.  \n4. **API 제공** – `localhost:5000`(또는 지정 포트)에서 REST / gRPC 엔드포인트 오픈.  \n5. **Open WebUI 연동** – UI에서 모델 선택·프롬프트 전송 → Model Runner API 호출.\n\n## 4. Open WebUI와의 통합\n### 4.1 Open WebUI 설치\n```bash\ndocker pull ghcr.io/open-webui/open-webui:latest\ndocker run -d -p 8080:8080 ghcr.io/open-webui/open-webui:latest\n```\n> 공식 문서: <https://docs.openwebui.com/>\n\n### 4.2 Docker Model Runner 플러그인 연결\nOpen WebUI는 `MODEL_RUNNER_URL` 환경변수를 통해 외부 모델 서버를 지정합니다.\n```bash\nexport MODEL_RUNNER_URL=http://host.docker.internal:5000\n```\n설정 파일(`config.yaml`)에 아래와 같이 매핑:\n```yaml\nmodel:\n  runner_url: http://host.docker.internal:5000\n  default_model: claude-code\n```\n\n### 4.3 인증·인가 연동\n- **API‑Key**: Model Runner는 `X-API-KEY` 헤더를 검증합니다. Open WebUI 관리 페이지에서 키를 생성하고 환경변수 `MODEL_RUNNER_API_KEY`에 전달합니다.  \n- **OAuth**: 필요 시 Open WebUI OAuth 플러그인을 활성화하고, Model Runner에 토큰 검증 미들웨어를 추가할 수 있습니다 (추가 조사 필요).\n\n## 5. 보안 샌드박스 구현\n### 5.1 마이크로VM 기반 격리\nDocker는 **Firecracker**와 같은 microVM을 활용해 컨테이너 내부에 별도 커널을 실행합니다. 이는 전통적인 컨테이너보다 **더 낮은 공격 표면**을 제공합니다 [Docker Blog, 2026‑01‑30].\n\n### 5.2 Docker Hardened Images\n- **무료 제공**: Docker Hardened Images는 라이선스 없이 사용·공유·빌드가 가능 [Docker Blog, 2025‑12‑17].  \n- **보안 강화**: 기본 이미지에 `seccomp`, `AppArmor` 프로파일이 사전 적용되어 있습니다.\n\n### 5.3 권한 최소화\n| 메커니즘 | 적용 방법 |\n|----------|-----------|\n| User Namespace | `--userns-remap=default` 옵션 사용 |\n| Seccomp | Hardened Image에 기본 제공된 `seccomp.json` 적용 |\n| AppArmor | `docker run --security-opt apparmor=profile_name` |\n\n### 5.4 네트워크 격리 및 데이터 영구성\n- **네트워크**: `--network=none`으로 외부와 격리하고, UI와 API 통신만 `--publish`로 허용.  \n- **볼륨**: 모델 캐시는 읽기 전용 볼륨에 마운트하고, 로그·설정은 별도 영구 볼륨에 저장합니다.\n\n## 6. 모델 배포 및 관리\n### 6.1 지원 모델 포맷·레지스트리\n- **Hugging Face Hub** (`hf://model-id`)  \n- **Ollama** (`ollama://model-name`)  \n- **로컬 파일** (`file:///path/to/model`)  \n\n### 6.2 모델 다운로드·캐시 전략\nModel Runner는 최초 요청 시 레지스트리에서 모델을 다운로드하고, `/var/lib/model-runner/cache`에 저장합니다. 캐시 정책은 `CACHE_MAX_SIZE`(GB)와 `CACHE_TTL`(days) 환경변수로 조정 가능.\n\n### 6.3 버전 관리·롤백\n- **버전 태그**: `model-runner:1.2.0` 등으로 이미지 버전 관리.  \n- **롤백**: 이전 이미지로 `docker compose down && docker compose up -d` 실행.  \n\n### 6.4 자동 업데이트(CI/CD)\nGitHub Actions 예시 (YAML 형식, 코드 블록 없이):\n```yaml\nname: Deploy Model Runner\non:\n  push:\n    tags:\n      - 'v*'\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Build Docker image\n        run: docker build -t ghcr.io/yourorg/model-runner:${{ github.ref_name }} .\n      - name: Push to GHCR\n        run: docker push ghcr.io/yourorg/model-runner:${{ github.ref_name }}\n      - name: Deploy\n        run: ssh user@host 'docker pull ghcr.io/yourorg/model-runner:${{ github.ref_name }} && docker compose up -d'\n```\n> 공식 CI/CD 문서: <https://docs.github.com/en/actions>\n\n## 7. 실전 예시\n### 7.1 Claude Code 샌드박스\n1. **이미지 Pull**  \n```bash\ndocker pull ghcr.io/docker/hardened/model-runner:claude-code\n```\n2. **컨테이너 실행**  \n```bash\ndocker run -d --name claude-code \\\n  -p 5000:5000 \\\n  --security-opt seccomp=default.json \\\n  ghcr.io/docker/hardened/model-runner:claude-code\n```\n3. **Open WebUI 연동** – `MODEL_RUNNER_URL`을 `http://localhost:5000`으로 지정.\n\n### 7.2 Gemini, Codex, Kiro\n위와 동일한 절차로 각각의 태그(`gemini`, `codex`, `kiro`)를 사용해 이미지 Pull·실행하면 됩니다. 각 모델은 **microVM 격리**와 **Hardened Image** 보안 정책을 자동 적용합니다 [Docker Blog, 2026‑01‑30].\n\n### 7.3 OpenClaw 로컬 실행\nOleg Selajev가 제시한 예시(2026‑02‑23)와 동일하게:\n```bash\ndocker run -d --name openclaw \\\n  -p 5001:5000 \\\n  -e MODEL=openclaw \\\n  ghcr.io/docker/hardened/model-runner:openclaw\n```\n- API 키 없이 로컬에서 실행 가능, 클라우드 비용 발생 없음 [Docker Blog, 2026‑02‑23].\n\n### 7.4 성능 튜닝 포인트\n- **CPU 제한**: `--cpus=\"4\"`  \n- **메모리 제한**: `--memory=\"8g\"`  \n- **GPU 할당**: `--gpus=all` (NVIDIA Docker)  \n\n## 8. 성능 및 비용 최적화\n| 최적화 항목 | 방법 |\n|-------------|------|\n| 리소스 제한 | `docker run --cpus`, `--memory` 옵션 사용 |\n| 자동 재시작 | `--restart=unless-stopped` |\n| 로컬 실행 | 클라우드 API 호출을 배제해 비용 절감 |\n| 모니터링 | Prometheus `node_exporter`와 Grafana 대시보드 연동 (Docker Hub `prom/prometheus` 이미지) |\n| 스케일링 | 필요 시 `docker compose scale model-runner=3` 로 다중 인스턴스 배포 |\n\n## 9. 트러블슈팅 및 FAQ\n### 9.1 일반 오류\n- **이미지 Pull 실패**: Docker Hub 인증 토큰 확인, 네트워크 방화벽 포트 443 개방.  \n- **포트 충돌**: `docker ps`로 현재 바인딩된 포트 확인 후 `-p` 옵션 변경.  \n- **GPU 인식 안 됨**: `nvidia-smi` 실행 확인, `nvidia-docker2` 설치 여부 점검.\n\n### 9.2 로그 분석\n컨테이너 로그는 `docker logs <container>` 로 확인. Model Runner는 `/var/log/model-runner.log`에 상세 스택 트레이스를 남깁니다.\n\n### 9.3 FAQ\n**Q**: Open WebUI와 Model Runner를 같은 Docker Compose 파일에 넣을 수 있나요?  \n**A**: 가능. `depends_on`을 이용해 UI가 Model Runner가 준비될 때까지 대기하도록 설정합니다.\n\n**Q**: 모델 업데이트 시 다운타임을 최소화하려면?  \n**A**: Blue‑Green 배포 전략을 사용해 새 컨테이너를 먼저 띄운 뒤, 트래픽을 스위치합니다.\n\n## 10. 향후 로드맵 및 참고 자료\n- **Docker Model Runner**: 향후 **GPU 자동 할당**, **멀티‑테넌시** 지원 예정 (Docker Blog, 2026‑02‑23).  \n- **Open WebUI**: 플러그인 마켓플레이스 확대와 **OAuth2** 표준 연동 로드맵 발표 (예정).  \n\n### 공식 문서·블로그·커뮤니티\n- Docker Blog – Sandbox & Hardened Images: <https://www.docker.com/blog/>  \n- Open WebUI GitHub: <https://github.com/open-webui/open-webui>  \n- Docker Docs – Engine & Security: <https://docs.docker.com/engine/>  \n\n### 추가 학습 자료\n- “Secure AI Inference with MicroVMs” – Docker Community Webinar (2026)  \n- “Self‑Hosted LLM Best Practices” – euno.news 시리즈 (2026‑01‑30)  \n\n> **※ 본 가이드는 현재 공개된 Docker Blog와 euno.news 자료를 기반으로 작성되었습니다. 구체적인 버전 호환성, GPU 드라이버 매트릭스 등은 추후 공식 릴리즈 노트를 참고해 업데이트가 필요합니다.**",
  "lastModified": "2026-02-26T00:25:00Z",
  "author": "SEPilot AI",
  "status": "draft",
  "isDraft": true,
  "isInvalid": false,
  "tags": [
    "Docker",
    "OpenWebUI",
    "LLM",
    "Sandbox",
    "Self‑Hosted"
  ],
  "history": [
    {
      "sha": "d3307da",
      "message": "docs: 뉴스 인텔리전스 기반 문서 자동 처리",
      "author": "GitHub Action",
      "authorEmail": "action@github.com",
      "date": "2026-02-26T00:25:00Z",
      "isAutoCommit": false,
      "additions": 0,
      "deletions": 0
    }
  ]
}